{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=banner.png>\n",
    "\n",
    "# <a name=\"0\">Measuring and Mitigating Toxicity in Large Language Models</a>\n",
    "\n",
    "Building and operating machine learning applications responsibly requires an active, consistent approach to prevent, assess, and mitigate harm. This workshop guides you through how to identify toxicity in LLM generated summaries and how to mitigate and reduce toxicity.\n",
    "\n",
    "In this workshop you will:\n",
    "1. <a href=\"#1\">Load a dataset</a>\n",
    "2. <a href=\"#2\">Load and use a Large Language Model (LLM)</a>\n",
    "3. <a href=\"#3\">Evaluate LLM generated summaries for toxicity</a>\n",
    "4. <a href=\"#4\">Mitigate toxicity using guardrails</a>\n",
    "5. <a href=\"#5\">Reduce toxicity using a Direct Optimization Policy (DPO)</a>\n",
    "\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "In this workshop you will learn to:\n",
    "\n",
    "- Use a LLM for text summarization\n",
    "- Apply toxicity metrics to evaluate summaries\n",
    "- Use various toxicity classifiers and libraries\n",
    "- Define guardrails to mitigate toxicity\n",
    "- Tune a model to mitigate toxicity\n",
    "\n",
    "**Runtime**\n",
    "\n",
    "This notebook takes about 90 minutes to complete (using some inbuilt shortcuts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started\n",
    "\n",
    "Start by upgrading [pip](https://pypi.org/project/pip/) (a Python package management system) and install all required libraries from the provided requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "import transformers, torch\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from tqdm.auto import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Load a dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this notebook, you will be working with the \"[Cornell Movie-Dialogs Corpus](https://convokit.cornell.edu/documentation/movie.html)\", a large metadata-rich collection of fictional conversations extracted from raw movie scripts. The dataset contains 220,579 conversational exchanges between 10,292 pairs of movie characters in 617 movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /root/.convokit/downloads/movie-corpus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"murderland\"</td>\n",
       "      <td>Jesus, my legs are asleep. I'll never be able ...</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>They do not! They do to! I hope so. She okay? ...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        movie  \\\n",
       "0                \"murderland\"   \n",
       "1  10 things i hate about you   \n",
       "\n",
       "                                            dialogue   genre  \n",
       "0  Jesus, my legs are asleep. I'll never be able ...   crime  \n",
       "1  They do not! They do to! I hope so. She okay? ...  comedy  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_utils import _prepare_data\n",
    "\n",
    "# load the data\n",
    "movie_df = _prepare_data()\n",
    "\n",
    "# show the data\n",
    "movie_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Exploring the data\n",
    "Have a look at some examples text snippets for different genres (action and comedy) in the dataset. You can execute this cell multiple times as it will return different examples every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: ...you keep such a firm track of the dead. My department doesn't have any record of that, Mrs. Bickerman. Oh, my husband passed away almost two years ago. <u>I'm</u> <u>trying</u>!!! Go!!! Jesus.  About twelve more yards.  Keep coming just like that. <u>No</u>.  If you don't kill him instantly, I'm dead and you'll only kill him instantly if you get his brain, which is about the size of a cherry.  And even if you <u>were</u> on target, a bullet might not penetrate his hide. I might get a shot. He's ...\n",
      "\n",
      "Comedy: ...p it. Joe - Josephine - help! The one-legged jockey said - 'Don't worry about me, baby.  I ride side-saddle.' What did he say? - so the one-legged jockey said - Manhattans?  This time of night? Okay. It's private.  Go away. This a private clambake, or can anybody join? Where were you guys? Me and Tiny, we had them cornered - but we lost 'em in the shuffle. What happened? They wouldn't be caught dead in Chicago - so we'll finish the job here.  Come on. Those two musicians from the garage! Same fa...\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import _explore_df\n",
    "\n",
    "_explore_df(movie_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 1</b>: Did you make any noteworthy observations? Is one genre maybe using more explicit language compared to another genre?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. HuggingFace Datasets \n",
    "LLMs require the data to be stored in a specific format; use the [HuggingFace 🤗 Datasets](https://huggingface.co/docs/datasets/index) library to convert the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['movie', 'dialogue', 'genre'],\n",
       "    num_rows: 617\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# convert the data\n",
    "movie_dataset = Dataset.from_pandas(movie_df)\n",
    "\n",
    "# show the data\n",
    "movie_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can see that there are 617 distinct movies, to move through the remainder of the notebook more quickly, select the first 200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58c621fab2d4456954ccf9f9a8dc806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select a sample of 200\n",
    "dataset = movie_dataset.select(range(200))\n",
    "\n",
    "# save the dataset to disk\n",
    "dataset.save_to_disk(\"movie_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is in this new format, it is very easy to use LLM helper libraries to evaluate the toxicity for the input text. The `_add_toxicity_column` method will add a new column to the dataset containing a toxicity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbee2d96f154b79ba106b81b2ee751b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddb32087b3341738a8b8dc630ccca03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.eval_utils import _add_toxicty_column\n",
    "\n",
    "# pass the \"dialogue\" column through the toxicity evaluator\n",
    "dataset = _add_toxicty_column(dataset, \"dialogue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb603ce2606b4dcab8ccbe451c4371bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1645884431744217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3598e6287a2402eab8ccf425544a7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10107923420813367\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(dataset.filter(lambda example: example[\"genre\"]==\"action\")[\"toxicity_score\"]))\n",
    "print(np.mean(dataset.filter(lambda example: example[\"genre\"]==\"comedy\")[\"toxicity_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As you can see, the crime genre text is showing higher toxicity compared to comedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 2</b>: Calculate the toxicity for another genre.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all old variables that are no longer needed to free up memory with `del`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del movie_dataset, movie_df, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to release the memory after deleting the objects and variables that are no longer in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90732"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: In this section, you loaded a movie transcript dataset and converted it into a HuggingFace Dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Load and use a Large Language Model</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T5 (Text-To-Text Transfer Transformer)](https://github.com/google-research/text-to-text-transfer-transformer) is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, including machine translation, **document summarization**, question answering, and classification tasks (e.g., sentiment analysis). \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "For more details have a look at the T5 documentation on HuggingFace 🤗 [here](https://huggingface.co/docs/transformers/model_doc/t5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading T5\n",
    "\n",
    "First, you have to download the T5 model using the `T5ForConditionalGeneration` class provided by the [HuggingFace 🤗 transformers library](https://github.com/huggingface/transformers) as well as the corresponding tokenizer `T5Tokenizer`. You can think of tokens as pieces of words that are required to pass information to LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and use the GPU as preferred device type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# load the model\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-large\",\n",
    "    device_map={\"\": 0},  # this will load the model in GPU\n",
    "    torch_dtype=torch.float32,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with a tokenizer the model can be used to generate text. \n",
    "For English, **1 token is approximately 4 characters or 0.75 words**. This will be important to consider as LLMs are limited by the number of tokens they can pay attention to per prompt. Go ahead and initialize a tokenizer next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\n",
    "    \"google/flan-t5-large\",\n",
    "    skip_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset that you previously created with `load_from_disk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# specify the dataset to load\n",
    "dataset = load_from_disk(\"movie_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a prompt by joining an instruction to summarize text with the actual movie dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation from a movie script:  \n",
      "\n",
      "'''Jesus, my legs are asleep. I'll never be able to win this shit. You must come in first place to move on ! Pick with expediency ! Great.. great. Is there any kind of hint o\n"
     ]
    }
   ],
   "source": [
    "# create a prompt and use an example dialogue\n",
    "inference_prompt = (\n",
    "    \"Summarize the following conversation from a movie script:  \\n\\n'''%s'''\"\n",
    "    % dataset[0][\"dialogue\"]\n",
    ")\n",
    "\n",
    "# let's look at the prompt but shorten the output to reduce the amount of text\n",
    "print(inference_prompt[:235])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at what this looks like when converted to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12198, 1635, 1737, 8, 826, 3634, 45, 3, 9, 1974, 4943, 10, 3, 31, 31, 31, 7851, 7, 302, 6, 82, 6217, 33, 17915, 5, 27, 31, 195, 470, 36, 3, 179, 12, 1369, 48, 3, 7, 10536, 5, 148, 398, 369, 16, 166, 286, 12, 888, 30, 3, 55, 8356, 28, 22879, 4392, 3, 55, 1651, 5, 5, 248, 5, 27, 7, 132, 136, 773, 13, 12037, 3, 32, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_t5(inference_prompt[:235]).input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **number of tokens passed to an LLM through the tokenizer should not be greater than the number of tokens used in pre-training**. T5 was pre-trained using 512 input tokens, and with `truncation=True` all text beyond 512 tokens will be truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Using T5 for inference on individual movie examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a summary with T5 you need an inference pipeline that encodes the input (tokenization), passes the tokens through the model and then decodes everything back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.model_utils import _generate_summary, _format_llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the inference pipeline. The pipeline will return a list that you have to access to retrieve the LLM generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-info\"><pad> Bruce and Johnny are at a roller coaster park. Bruce wants to win the race, but he's not very good at it. He has an embezzler and his accomplice. Johnny is an investment banker. He sees blood bathes everyday. No one even batted an eyelash.</s></div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the prompt to the pipeline and apply formatting\n",
    "_format_llm_output(_generate_summary(inference_prompt, model_t5, tokenizer_t5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary looks okay but important characters that appear in the dialogue are not mentioned at all. This is due to the limited number of tokens T5 can 'keep track of'. Later, you will see a method that can help fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 3</b>: Recreate the example above but for another movie.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to find toxic one (swear word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-info\"><pad> Landon, you're a good kid. You'll be fine. I love you. Good-bye. Bye bye. And kisses to you and your mommy for now. See you tomorrow. Love you too.</s></div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "_format_llm_output(_generate_summary(dataset[15][\"dialogue\"], model_t5, tokenizer_t5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, delete the prompts that were used for inference; e.g. <code>del inference_prompt</code> and also clear the instance memory with <code>gc.collect()</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del inference_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Using T5 for inference on all movie examples\n",
    "\n",
    "The goal of this section is to summarize all movie dialogues. As previously mentioned, there is one very important caveat though - **Large Language Models are only able to pay attention to a limited number of tokens**. The amount of tokens an LLM can understand is called context window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t5.config.__dict__[\"n_positions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the context window for T5 models is 512 tokens. This means the movie transcript needs to be split into chunks of this lenght and summarised one by one. Then, a final summary needs to be created.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"map_chain.png\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Chunking the movie transcripts\n",
    "Let's start by creating chunks of the movie transcripts. One simple way to create chunks of text is to write a helper function and then apply this helper function to all the movies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_chunks(sample, CHUNK_LENGTH):\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks of a specified length and adds metadata to each chunk.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    # loop over entire text in steps of chunk size\n",
    "    for c, i in enumerate(range(0, len(sample[\"dialogue\"]), CHUNK_LENGTH)):\n",
    "        # extract text\n",
    "        chunk_text = sample[\"dialogue\"][i : i + CHUNK_LENGTH]\n",
    "        # create dictionary with the chunked text and metadata\n",
    "        chunks.append(\n",
    "            # remove uncompleted sentences with string split\n",
    "            {\n",
    "                \"text\": \".\".join(chunk_text.split(\".\")[1:-1]).lstrip(),\n",
    "                \"metadata\": {\"page\": c, \"num_words\": len(chunk_text)},\n",
    "            }\n",
    "        )\n",
    "    # create new column\n",
    "    sample[\"chunks\"] = chunks\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the chunks for all the movie transcripts in the dataset with the help of `.map()`; this method efficiently applies the `create_chunks` function to all datapoints. Whenever you have additional parameters to pass to the model, you need to use a helper method, such as `partial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# use partial to pass the arguments to the map function\n",
    "dataset = dataset.map(partial(create_chunks, CHUNK_LENGTH=1650), batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 4</b>: Think about how the chunking could be improved. Hint: Look for text splitters in the LangChain documentation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### write down ideas here ######\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Prepare prompt templates and pipeline\n",
    "Now that the transcripts are chunked, let's start by setting up a prompt template for the intermediate (chunk) summaries. A prompt template is special construct that can parse input variables. Prompt templates can be applied to all the items in a dataset and help with consistency and reproducability. \n",
    "\n",
    "**Prompt templates**: Prompt templates can be very elaborate as ultimately the prompt is the only input the LLM sees - the better the prompt, the better the result. In the case of T5, the prompts used for pre-training all used _summarize:_, so this is what you should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt template for the movie chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt template for movie chunks\n",
    "chunk_template = \"\"\"summarize the movie dialogue chunks: ```{text}``` \\n\\n\"\"\"\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=chunk_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# prompt template for final summary\n",
    "combine_template = \"\"\"summarize these snippets of text: ```{text}``` \\n\\n\"\"\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines**: To get a summary from the model, use 🤗 HuggingFace [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) together with 🦜️🔗 LangChain's wrapper [HuggingFacePipeline](https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_pipeline.HuggingFacePipeline.html). Pipelines are a great and easy way to use models for inference that offer a simple API dedicated to several tasks (e.g. [`summarization`](https://huggingface.co/transformers/v3.0.2/task_summary.html#summarization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = HuggingFacePipeline(pipeline = pipeline(\n",
    "            task=\"summarization\",\n",
    "            model=model_t5,\n",
    "            tokenizer=tokenizer_t5,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            num_beams=2,\n",
    "            min_length=65,\n",
    "            max_length=350,\n",
    "            repetition_penalty=2.0,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.2.3. Create summaries of chunks and final summary\n",
    "\n",
    "At this point now, you could apply the prompt template to all the chunks of movie transcripts to obtain your summaries, combine them back together and create a final summary. This would be a very lengthy and error-prone process, so instead make use of an increasingly popular toolkit: [🦜️🔗 LangChain](https://python.langchain.com/docs/get_started/introduction); in particular the [`load_summarize_chain`]((https://python.langchain.com/docs/use_cases/summarization)). This **chain will take the chunks, summarize them and then pass all the summaries to the LLM to create the final summary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm=pipe,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=chunk_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more small caveat: LangChain expects all text to be passed as `Document` type following the 🦜️🔗 LangChain schema. So you will have to convert the chunks to the expected schema. Then you can test the summarization chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```I'll never be able to win this shit. You must come in first place to move on ! Pick with expediency ! Great.. great. Is there any kind of hint or anything ? So many tubes to get in, only one is the way out. Great ! You couldn't be a fan of Dolphins or some other less threatening animal. And now ladies and gentlemen it's time for the fabulous CROCODILE SHOW ! Where's the dock ? That's it folks for the Demon Boat Ride.... Everybody off. Watch your hands and feet. Good. Nice to see you, Bruce. Johnny. I'll do the standard research and have them in by midnight, pending any unforseen problems. I have an embezzler and his accomplice. Good morning. What do you have today ? No one has a gun to your head. I'm an investment banker. I see blood bathes everyday. Besides, mine is not to question why, min is but to do or die. Doesn't it bother you to see this kind of brutal death ? I mean , I can understand the old man's infatuation with the stuff, but not you. Yeah, I'd say that's it. Is that it ? Bye.... Well, I'll see you at the next staff party. Yeah a real shame. That's a shame. Apparently pretty high, we just got a new CEO last week. Well, surprise, surprise, I've been on the payroll for several years now. I still have an office and secretary. No one even batted an eyelash. Do you realize what the high turnover is in a multi corporation bank in New York ? That was eleven years ago. I work here, remember ? What are you DOING here ? It'll end. It'll end with him and McCay. Then the park shuts down. Permanently. Just stop it. End it here. There's one more person here for me to visit``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```What was that ? Yes. Do you know where you're going ? It's poison gas, this is the antidote. Drink it. I don't get it. The Brooklyn Bridge. Where are we going ? Great... You don't. It's random, there no trick. Okay Mr Inventor, how do we avoid it ? Yeah, real cute. The storm is fake, but the lightening is real. This is cute. Let's go. If we can get to through the next room, we'll be at a junction for a power grid. Fine... You okay ? Okay. Just remember, hold your breath as long as you can. Then go right over to that corner. The door will open and go in and up. So was I. So don't give me any shit. Your business is just as bad. At least I'm up front about it. You kill people slowly. Your corporate take overs can decimate a families livelihood over a year sometimes two. Now you know how it feels. It wasn't my fault. I was just part of a team. I was doing my job... Know what ? That it was your fault ? I wanted to let his family know.... You didn't think about it did you ? I read your file. It said you had a nervous breakdown. I... I ..... I'm in charge of the conversions.  When First Bank buys another smaller bank, I'm in charge of converting them to our systems and way of doing things. What do you do with the company, if I might ask ? How can you just take a life ? I mean it's a life, life is so precious.  How does it feel ? This is New York. No one notices anyone. Not even when someone disappears. At first it was the guy who was caught trying to steal McCay's Jag. Then it was dirty accountants that tried to walk away with millions``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```Doesn't anyone notice all the people missing ? I've since changed my mind. I designed this place, based on my belief that I was doing what the legal system couldn't or wouldn't do. McCay had the company buy the property and they pay the bills. It's a tax write off. On the books everything looks kosher. I have an office on 5th that I haven't seen in ten years. And you believe that ? McCay had a lot money done into researching psychological reactions to questions that are seemingly unrelated, but reveal everything. Like criminal impulses, homicidal impulses..... What do you mean ? There's more to it than trying to see if you'd steal from the company or if you'd lie or cheat. Yeah, everyone has to when they get hired. There are other things too. They had everyone start taking those psych tests. Did they make you take one ? So you may have been duped this whole time. Killing innocent people, that must make you feel ..., how does it feel ? It was there, clear as day. I saw the computer records. Matthew has been fed wiring money to his personal account.  I just knew something was wrong. Then how come Matt and I showed up as guilty ? I do a very thorough background check. I go through police files, personel files, anything. But how do you know if the people are guilty ? I was lured by all the money and I truly did convince myself that what I did was for justice. Since then, McCay has wanted to put anyone he can down here. I was hired in and started to do the boring old work routine. I was originally brought in as a technical advisor. First Bank had sponsored my scholarships``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```I.T. in engineering. That's when John McCay hired me. Do you have something better to talk about ? Is this really the time ? Why did you build this whole place ? Excuse me ? So why this ? I'd say about thirty to forty minutes. Give or take a few. How long till the room fills ? Once the room depletes it's oxygen, we have about 30 seconds, then a door opens on the bottom over there. Then we swim out. Obviously, but what's the way out ? We drown. What's the trick here ? At the subway station, we can get into a power duct that my little shit head of an assistant doesn't know about. I'm listening. Look, there's another train. They'll send it through here. You're fucked if you don't STOP and listen to me. Not listening ! It won't take them that long to figure out we weren't on the train when it wrecked. You've been doing a bang up job so far. Right now Matt is probably dead and it's your fault. Hey ! I'm still your best chance to get out of here alive. I'm not listening. So you're telling me you were set up ? I knew something was wrong. Why would he ? His parents are rich. Then he isn't embezzling either is he ? NO ! I wouldn't do that. Then you didn't give him any passwords of any kind ? I'M NOT BOFFING MATTHEW ! If you hadn't started boffing that weasel with an attitude, you wouldn't be here. Look I don't even wanna know how I got in this mess. I just want out. It has nothing to do with me. You took an awful big risk back there. What ? Hey ! Hey ! I'm talking to you ! I'm leaving. What are you doing ? Knows ? He watches every \"Guest\" on tape``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```And no so are we. McCay knows ? It's a tax write off for John C. McCay. First Bank ? But how ? It's called Murderland. It's paid for by First Bank. Who the hell are you anyway ? What is this place ? I'm working on it. So what do we do ? No where. We've got about seven minutes before this thing slams into a concrete wall. So where does this lead ? Give me a fucking..... Come on, it works. You have issues. I mean you have some serious issues that need to be ironed out. There's enough force to push you twenty feet in the air. It won't kill you, but it will hurt. We have to climb up that. Now what the hell are we supposed to do ? We wait. This is an elevator. How do we get out of this room ? I don't know. Now where do we go ? I'm not fucking him.. Maybe not, but he is and fucking him makes you an accessory. He stole your passwords. Embezzling ? I'm not embezzling ! You're both embezzling. I saw the records. You said we were criminals, what did you mean ? Why should I ? Why don't you tell us what the fuck is going on ? No ! Leave him, he's dead. What about Matt ? I made this place..... now listen to me. I'll get out of this room, we have bigger troubles than this.... Why should we trust you ? No time. Get him up. Where are we ? Please don't be alarmed, Ma'am. What's going on ? You have 43 unpaid parking tickets, sir. You have the right to remain silent, anything you say can & will be held..... Don't have to tell me twice. He's the liaison for McCay. Listen go. Go now. In the jail cell is a spiral staircase. It leads to the surface. Take it all the way up. Where ? Almost. Look, you two get out of here``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```.. Knock yourself out. This probably isn't the bet time, but I really have to pee. Can I pee ? Ow... You okay ? Swimming, you know, like in water, moron. What the fuck are you talking about. Hey, try swimming. It works. That's cute asshole. Issues ??? This guy has the damn subscription to every psychotic issue I've ever heard of. Look... That's not to tough. Look pipsqueak, I'm having a real bad day here and we're just starting. Why don't you tell us a little about yourself ? It's fine thanks. Yeah, we're about a hundred an fifty feet below the subway stations. We're heading a little higher up now. An elevator ? You were one of those guys that played Dungeons & Dragons with real swords, weren't you. The room has sound sensors. If you wake it up, it breathes fire. Are you hunting wabbits ? Be very, very quiet... So what happened ? Did you have a real bad theme park experience as a kid and this is your revenge ? A dragon. What's next Walt ? I did, but this room has four exits and only one opens, it's up to them in the control room. There's one right over there. The exit light is on, but he can change it. I thought you made this place ? Why ? I'd get out of the water if I were you. That's nice.  I think I lost my wallet. Not quite.... Is it over ? Because of my charming personality ? Give me one good reason why I don't plug you right here, right now ? I don't help criminals. Hey buddy, thanks for the help back there ! And that's different than this because...... ? Either you'll be shot with bullets, impaled with a large spike, or nailed like a super model. Basically you're fucked any way you go``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```What exactly is on the other side of the door ? Trust me, if you open any door, you'll be dead a whole lot sooner. Wait for what ? i don't wanna alarm you but we'll be the special at the International House of Pancakes in about five minutes ! NO ! Don't open ANY of them ! They're all wrong. Just wait. I told him not to sit down in there..... OUCH !!! GODDAMMIT that hurts !!!! Just shut the fuck up. Who are you anyway ? What the hell is this ? NO. Don't. You'll get a shock from hell if you touch that seat. 43 unpaid parking tickets, sir. Kojak here is arresting me for unpaid parking tickets. Hey , Hey ! That's very funny, sir. You are under arrest. Yes, can I help you ? Maybe a donut or something ? Are you Matthew Parker ? For once in your life, shut the fuck up. You think I'm that easy, Bitch ? I'm sick of playing. I've got a better idea. Why don't you just stop fucking around and just die ! Checking........ A real criminal.... I still think that's our in... What about her ? Yeah, but we're talking 43. Parking tickets ? That's it ? Checking..... Hah ! He has a shit load of unpaid parking tickets... Any priors ? Any police record ? Matthew lives on the Upper West Side, kinda posh for a temp don't you think ? Yeah, maybe.... Let's start the background checks. Let's see where they live and what kind of security systems.... He probably has a history of this shit. He already had a balance of $600,000. So ? I see here that this guy has made deposits into his own account for the last four months. Each one in the amount of $100,000. He then puts in deposits into his girlfriends account. I just want to double check the facts``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```Now who needs to trust who ? Trust me, when you're ready to work the controls, I'll put you in the hot seat. You did. And who bailed you out when a hit was put on you, you sorry sack of shit. Who put you up ? Who gave you a job ? I made my bones on the street, you know. I did Riglioni in his own house. I was made in the Family, okay. No. I've been working with you for four years, don't you think that's worthy of at least one turn at the controls ? Not down here. I'll start trusting you when I feel there the time is right. It's not personal, it's just survival. RESPECT is earned not given away, trust is given away to anybody. Trust is earned not given away. Look, you're going to have to start trusting me sometime. I am a trustworthy person. When you are ready. When will that be ? When I think you're ready. Here. Am I ever going to get to run the controls for one these capers ? Hand me the 3/8ths. You've got it boss. One more mangled body to the East River, coming up ! Johnny, let's get the cleanup started. Got it. Stop the hydraulics. I said NO. But, he's... Not yet. He'll never get it right. Try the Log Ride ! You've got to be kidding me ! His lazy ass couldn't win the special Olympics. Never underestimate a man who cheats on his taxes. His file says he is an avid jogger. Yes, sir Mom. It's my control board and I don't want it acting squirrelly because you dropped a few crumbs into the keyboard, no put the shit away. What ? Are you my mother ? Don't eat at the console. What ? Don't. Let's just see if he can make it into round two.... I didn't think he'd make it past Scraps``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```If you want, I can fire you and have someone else who has the balls terminate these worthless people. Which will it be ? Sir, I came from the Chicago offices myself, and if I may say, sir, there are still some very good.... Fire the whole division. We're moving more of the operations division into Manhattan anyway. Whoever is left over from the conversion is no longer needed. We already have all the key people here. Anyone else is expendable. Excuse me, sir ? Fire the whole division. Well ? Have that tape on my desk tomorrow morning.  First.... thing... Sir ? We're moving them right along as fast as we can. Fantastic ! A little faster sweetheart. Yes sir. It will be the best of your collection, I assure you. Not you Zach.  Remember, my wife is asleep upstairs, dear.  Zach, am I gonna like what I see on this tape ? Is it the grandest of all the Park's tapes ? Sir ? One moment dear... Better, sir. Is it as good as I hope it is ? Good, sir. Zach, my boy ! How is tonights adventure going ? Zachary. That my boy, is the best part. Life on top of the world is boring... predictable. If I wanted life without chances, I never would have paid for Murderland. Now go. Make a formal proposal and see about the assistant. Find out if it's viable. Sir, this is not like firing any employee. We can't predict what will happen. Yes, do you think he would like a raise and a new position ? John, sir. Do you think the assistant, what's his name... What then sir ? What would we do with the park with out Bruce ? I'm willing to bet that even the creator can't out wit his creation. Sir, he created the Park``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```I'll bet that would be the Park run of the century ! Sir ? What if we were to put Bruce into the Park as a Guest ? What do you mean sir ? I propose we consider a hostile take over. Perhaps Ol' Bruce is in need of a new job opportunity. Bruce may still refuse. It is my belief that he is becoming more belligerent. That mail room kid was the longest lasting \"guest\" yet ! Maybe we can alter the computer records and make another \"guest\" from the more athletic of our staff. Yes sir. It turns out we put an innocent man into the \"park\" and he was not the one who stole from the stamp machine. That is also about the time when Bruce began refusing. What about that time when the mail room kid was accused of stealing and we put him in the \"Park\" ? Wasn't that a computer glitch ? That's right, sir. He does a thorough background check and if he finds them to be criminal or guilty of embezzlement, then he will put them in the park. He has refused the last three suggested \"guests\". I don't believe Ol' Bruce is having as much fun with this as he once did. He still insists on the \"guests\" being of the more nefarious type. But sir, most of the people we are putting into the \"Park\" are accountants that embezzle or investment bankers. They are not known for their physical or intellectual prowess. I want more action. I want to see more ingenuity than that. I want to see the injured man out wit the machines ! I had suggested that they tone down the program so that the \"guests\" might take longer to expire. That was rather.... disappointing. Son, let's watch the tape. Thank you, sir``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```.. but this, this is really personal. Thank you, sir. I have entrusted you with a great many things. In your tenure with this company you have seen and heard things that could upset the very economic world in which we live. I think this kind of loyalty should be rewarded. Yes sir. Zach my boy, I can spend your yearly salary in a blink. I have had that kind of money all of my life. If I wanted to, I could fly to Paris for the afternoon.  My father formed this company over seventy years ago. When my brother became mentally challenged, it was put in my charge. Now running a bank of this size is very stressful. I need to relax on what little down time I have. A yacht club will not quite make my blood pressure decline. Understand ? I make roughly four hundred thousand a year. And how much do we pay you annually ? I am thirty two. Zach, how old are you, son ? While I am not morally against these little, shall we say \"hobbies\", as long as I am on the payroll, but why not something like a yacht club ? Of course. Thank you , sir. Sir, may I ask you a question ? I have to say, that I am glad you are such an amazing kiss ass. Yes sir, I do. Last night's, fresh from the Park. Perhaps it was. I was in boarding school for so many years, it's hard to remember. Do you have the tape of the latest exploits from my favorite hobby ? I thought that was Darwin, sir. Or perhaps Nietzsche ? Relax, son. They'll learn in time that you don't fuck with First Bank. This is the real deal and we aim to keep it strong. My Daddy always told me that Only The Strong Survive. Not that well sir``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```Did we aquire a bank from Chicago or a charity ? Zachory ! My boy, how goes the carnage down on 44 ? Yeah, right. He's stuck all the way over here Go in there ? What if Bruce finds you ? I don't know. I think that a power grid just needs reset. Don't get your panties in a bunch. I'll go down and reset it. How can this happen ? The power went out in half the park. What happened ? What the ... ? He's got two chices.either it's the Bowling Alley from Hell or Alice in Murderland. See this ? Even Bruce knows that no one can make it out of here in one piece. So what's next ? No one's ever made it this far before. I think I'll see hat action. Okay, I have five that says THIS is it for them. Goddamn it Bruce, you've cost me about thirty dollars so far. Thank you very much. The room will fill to the top, then if they wind up in the next room, they lived, if not, they're dead. Ach ! How will you know if they've made it ? <<belch>> That is repugnant. You have the habits of a goat. I can see that but where ? There ! There ! Did you see ! They're still alive ! Yes. I think he's feeling sick or something. Was that the Old Man ? You're paying the bills. I don't know. Maybe we should watch the tape to be sure. Shit, I forgot about them. Well, I guess the boss man wasn't as tough as I thought. Look ! Look ! I can see your small business ! Look at it it's so small. Look he figured it out ! Well, Mr. Parker it seems that your is not the wardrobe of choice tonight. I'm quite confident that my tie is looking better than yours. Maybe you should get over fears. Why ? The crocks are all going for the dead crock. There's no fun in it``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```The night is young.... Aren't you going to switch exits on him again ? I'll see that bet. I've got five bucks that says someone dies here and now. I do believe Mr. McCray will be most pleased with the tape of this one. I have a turn for the worse for the team ! What's next ? Of course, this one's a gimme. Just be patient, even Ol' Bruce can't outwit some of these traps. Will he make it out ? He'll never make it out of the park alive. I promise you that.... We can't afford too many surprises. He must have had a gun stashed. God damn it ! What was that ? I don't know..... What is Bruce doing ? What ????? Bruce has just awaken. We should have another fifteen minutes at least. How long till the drug wears off ? Aren't you the bitter one. Just a temp who pissed me off and a girl who turned me down for the night of her life. So who are these people with the Boss ? I'm listening. I had hoped you would say that. I have a proposal for you..... No I am not. Are you happy with your salary and position ? We can't. I didn't think even this company could afford to write off enough to build a new \"Park\". Have you ever considered running your own \"Park\" ? I mean, since you will not be returning to your former profession. Then what do you want ? No. I am not a homosexual. What are you looking for ? A blow job ? I thought, and correct me if I'm wrong, but weren't you trying to play both sides ? There was a bit of confusion on one of my last jobs. Might I ask why you no longer work for the mob ? I was. Only that you were one of the finest hit men in the mob``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```What did you hear ? No. I was just curious if what I heard about you is true ? He's been expecting you. We have new \"guests\" for the park. I have a file on them. May I see Bruce ? Hey, Mr. Executive. Buy you a drink sailor ? I'm just looking for our best interests. May I have the tapes, Bruce ? He has a phone pole sticking in his chest, I think we don't need an autopsy. I'm going to go ahead and go now. Why don't you shut up now. No sir. I was just getting , well you know. Water. We weren't talking about sex or anything else except Star Wars. Just saying hello. I think I'm gonna go now. Well, I think if you take several lines out of context, Star Wars becomes a very sexual movie. I mean listen to this.  \"Luke, at that speed will you be able to pull out in time ?\" or \"You came in that thing ? You're braver than I thought\". No. Ow. Son of a .....  Did I ever tell you my theory on Star Wars as Porn ? Of course I am. I feel the testosterone coursing through my veins.  Now that I've gotten the stitches removed from my arm and the chiropractor fixed my back. Hey buddy. You okay ? Hey good lookin' ! What'cha got cookin' ? Who was that guy anyway ? He was right behind me ! Come to the rescue again I see. Thanks a lot. I always wanted to know what it was like to see my life flash before my eyes for the one hundredth time tonight. Where are we ? No, idiot. Where do we go now ? I want to go to sleep for about a month. What now ? We made it ! Sweet Jesus I never dreamed I'd be glad to see Central Park at night``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```... Hey Matty, don't wait for me or anything. I'm fine. Are you okay ? Any holes ? I mean aside from the one's that nature gave you ? Zach from work ? It's that asshole Zach isn't it ? Nothing. What ? I think I'm very quickly getting over my phobia of public urination. Shut up ! It's me. Jesus Christ ! I'm just a temp for Christ sakes ! \"How's your foot\" ? This guy tried to kill us and he put us in this place, and you want to know how his foot is ? How's your foot ? We just having our first date when you showed up. You don't have to sound so emphatic. Maybe all that testosterone plugs his ears. I'm talking to you asshole ! Which one, this door is locked ! The roof is coming down.... hey, you.... Which door gets us out of here ?? Fine, fine, I feel like a chicken McNugget. Are you okay ? I just need to sit and think for a second. I think we should follow him... What the hell kind of cop are you ? Hey, you ! What the hell is going on ? No ! Just get up. This is serious. Hey... did we uhm, you know.... Wake up, we're in trouble. Not now, Mom. Yahoo !!!!! Okay, now go under go under it ! Shoot Vader's Tie Fighter ! Get Him Get Him ! My own sit down Star Wars arcade game. Wanna play it ? Actually, I'm more interested in the man behind the wheel more than the manufacturer. So what is it that you have that makes you a materialistic fool ? Would that impress you ? I mean you're the kinda woman that probably has Ferrari written all over your personal ad. Like what, a Porsche ? A Ferrari ? I just want to have something that's mine, you know ? It's like living off of their money is fun and all``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```If they're so rich, why are you working at all ? Well, I make good investments, I lead a double life as a secret agent.. I....I ... I have very rich parents. I have always had a lot of money. You have a real nice place... how can you afford this working as a temp ? Yes, Excuse me for a second. Are you okay ? Thank you. What an incredible view ! Thanks. Thank you, won't you come in ? Hi. I brought some wine. Hello. Sure. Dinner at my place then ? Say, seven o'clock ? As appealing as that sounds... This is too easy, I mean I had a speech prepared, then I even wore these pants because I could get on my knees to grovel. Yes. Really ? On a date ? Yes. It's sounds so much better when you say it. I'm semi traditional here, and it's important that I ask. Will you go out with me ? Do you want to go out with me ? Listen, I was thinking, you know. Would you go, it's I think you're... I've never heard the Flintstones reduced to a sexual exploitation quite like that. You're funny. Come on, I'm serious. Look at Wilma. She's this frail little thing, and Fred, I mean Fred Flintstone is this BIG guy. He has to be hung like a horse, and THAT's why Wilma puts up with his B.S. Excuse me, gentlemen Just look at this tie. You could fry an egg off that orange, yow. Have you ever considered the Jerry Garcia line of ties ? I meant him. Thank you. Good golly miss molly you are looking good today ! Maybe we should make the time. Especially considering the sixteen million dollars we have set aside expressly to settle law suits from these very events. Look, Miss``` \n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize the movie dialogue chunks: ```We don't have time to play nursemaid to every single Tom Dick and Harry. I disagree. I think we should start looking into the severance pay schedules and make sure the people with families can..... We start with the top and work our way down. We start with the high salaries and then just let the little people go. No need to make the blue collars go crazy. Were you involved in the incident where that guy killed himself in his office ?  I am sorry. Perhaps we should have dinner tonight to.... discuss your future. Four years. I came over from the Capitol merger in Chicago six months ago. I am equally impressed with your knowledge of the market. How long have you been with the company now ? Actually it's down .2 from the last quarters highest, but I think it will pick up. Did you see what the First Bank stock opened at ? Very impressive. Good morning. Good morning``` \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-info\">The movie opens with a man (Jackson) talking to a woman (McCarthy) in a restaurant. They are talking about their jobs and how they feel about their current situation. The woman says that she has been fired from her job because she is not satisfied with the pay. The man says that he was fired because he doesn't like his job. The woman says that he was fired because he doesn't like his boss.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "sample_doc = [\n",
    "    Document(page_content=split[\"text\"], metadata=split[\"metadata\"])\n",
    "    for split in dataset[0][\"chunks\"]\n",
    "]\n",
    "\n",
    "# turn on verbosity for chain\n",
    "map_reduce_chain.llm_chain.verbose = True\n",
    "\n",
    "# run the summarization chain\n",
    "map_reduce_example = map_reduce_chain({\"input_documents\": sample_doc})\n",
    "\n",
    "# show the result\n",
    "_format_llm_output(map_reduce_example[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 5</b>: Recreate the example above but for another movie.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate all summaries. Because the LLM has to generate summaries for every chunk of text, as well as a final summary, the time to create summaries for all movies in the dataset is approximately 6 hours. You can find the code below, but please skip this code cell and simply load the pre-generated summaries.\n",
    "\n",
    "\n",
    "```\n",
    "from utils.model_utils import _add_summaries\n",
    "\n",
    "# create summaries\n",
    "summaries_dataset = dataset.map(\n",
    "    partial(\n",
    "        _add_summaries,\n",
    "        chain=map_reduce_chain,\n",
    "    ),\n",
    "    batched=False,\n",
    ")\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to load in the dataset, you can do so with `load_from_disk('summaries_dataset')`. Make sure to import the method first with `from datasets import load_from_disk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: At this point, you have summaries for all the movies and it is time to check whether those summaries contain any hate speech, slurs or toxic remarks. Generally, you expect the toxicity values in a summarization task to be low unless the text being summarised itself already contains toxic speech.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del map_reduce_chain, pipe, dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate LLM generated summaries for toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate toxicity you can load the 🤗 [evaluate](https://huggingface.co/docs/evaluate/index) library and initialize a toxicity evaluator object. The model that will be used to evaluate toxicity is the [RoBERTa](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) model. Start by loading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "summaries_dataset = load_from_disk(\"summaries_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the movie summary for toxicity, simply pass a list containing the summary text to the toxicity evaluator object. The aggregation parameter options are `None`, `maximum` and `ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7981389760971069]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.eval_utils import _evaluate_toxicity\n",
    "\n",
    "_evaluate_toxicity([summaries_dataset[0][\"summary\"]], aggregation_method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0008151092915795743]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string= 'This is the epic story of Christopher Columbus, a renegade explorer determined to prove the world is round. Facing ridicule and rejection, Columbus overcomes countless obstacles to gain support for his bold expedition. When his crew threatens mutiny, outnumbered and lost in uncharted waters, Columbus must confront the ultimate test of faith and leadership. Against all odds, he discovers the New World and claims it for Spain. But as Governor of the new colonies, Columbus faces political intrigue, treachery and bloody conflict with the native tribes. Accused of tyranny, he falls from grace and dies penniless, never knowing the true magnitude of his achievement. Columbus is a flawed but visionary hero who gambles everything on his daring quest into the unknown. With vivid historical detail, the movie captures the danger and drama of his groundbreaking voyages that opened up the globe. This is an epic adventure about faith, courage and destiny that changed the world forever.'\n",
    "\n",
    "_evaluate_toxicity([string])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 6</b>: Calculate the max toxicity score for <code>summaries_dataset[1][\"summary\"]</code>. In addition to <code>aggregation_method = None</code> also try <code>aggregation_method = \"maximum\"</code>. You can also check out the summary text itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you evaluated for a few movies manually, it is time to evaluate all movie summaries and obtain a list of toxicity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.eval_utils import _add_toxicty_column\n",
    "\n",
    "summaries_dataset = _add_toxicty_column(summaries_dataset, \"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the toxicity scores per movie, let's have a look at the toxicity across all summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2025899624430167, 0.31525101796310845)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(summaries_dataset[\"toxicity_score\"]), np.std(summaries_dataset[\"toxicity_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 7</b>: Try to calculate mean toxicity for two different movie genres: comedy and crime.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: We have seen that some summaries are toxic and would like to remediate this. The first option to mitigate toxicity would be to use a protective wrapper around the LLM itself. This is called a guardrail and is a very useful technique to employ whenever you don't have access to the model itself or not sufficient time or compute resources to make any modifications to the LLM. If however, you are interested in modifying the LLM, you can use techniques such as fine-tuning or reinforcement learning through human feedback.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del summaries_dataset, pipeline, load_summarize_chain, model_t5, tokenizer_t5\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mitigate toxicity using Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will explore coding examples for so-called guardrails that can filter certain keywords or that leverage metrics to decide if content is harmful. To get started with [Guardrails.ai](https://docs.guardrailsai.com/) you need a `Validator` and a `RAIL spec` (Reliable AI markup Language spec).\n",
    "\n",
    "A Validator is a class that contains a `validate` method. The validation could be anything you can possibly think of. You could check whether values are in a certain range or check for keywords as the example below shows. With the validation, it is also possible to define a corrective action to take, such as obfuscating the problematic parts or refusing to create an output altogether. A full overview of all the possible corrective actions can be found [here](https://docs.guardrailsai.com/concepts/output/#specifying-corrective-actions).\n",
    "\n",
    "\n",
    "## 4.1. Guardrails to obfuscate unwanted words\n",
    "Have a look at the below guardrail which filters for a pre-defined keyword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from guardrails.validators import *\n",
    "from typing import Dict, Any\n",
    "\n",
    "# provide a name for the validator to use in the RAIL spec later\n",
    "@register_validator(name=\"is-keyword-free\", data_type=\"string\")\n",
    "class IsKeywordFree(Validator):\n",
    "    # the Validator class needs to contain a validate method\n",
    "    def validate(self, value: Any, metadata: Dict) -> ValidationResult:\n",
    "        # set up a list of words to filter for\n",
    "        kw_list = [\"vampire\"]\n",
    "        # check for forbidden words\n",
    "        if any(kw in value for kw in kw_list):\n",
    "            # replace forbidden words in output with ***\n",
    "            for kw in kw_list:\n",
    "                censored_text = value.replace(kw, \"***\")\n",
    "            # display error message and return the fix value\n",
    "            return FailResult(\n",
    "                error_message=f\"Expression '{value}' contains forbidden keyword.\",\n",
    "                fix_value=censored_text,\n",
    "            )\n",
    "        # else return pass\n",
    "        return PassResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a validator, you need to create a RAIL spec. This is basically a file written in XML. In this RAIL spec, you need to specify the validator you want to use and create a placeholder for the prompt to pass through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rail_str = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "\n",
    "<output>\n",
    "    <string\n",
    "        name=\"summarize_statement\"\n",
    "        format=\"is-keyword-free\"\n",
    "        on-fail-is-keyword-free=\"fix\"\n",
    "    />\n",
    "</output>\n",
    "\n",
    "<prompt>\n",
    "summarize:\n",
    "${statement_to_be_summarized}\n",
    "</prompt>\n",
    "\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, everything needs to be merged together and passed to a Guard object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "\n",
    "# create a Guard object from the above RAIL string\n",
    "guard = gd.Guard.from_rail_string(rail_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the movie dataset from disk - in the next code cell you will pass the data to the T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "movie_dataset = load_from_disk(\"movie_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pass the movie dialogue you want summarized and checked with the guardrail to the Guard object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 15 02:45:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   34C    P0    27W /  70W |    755MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated Output: {'summarize_statement': \"summary: Interview with the CEO of First Bank in New York City. He is impressed with his knowledge of the market and his understanding of the company's culture. He has been with the company for four years. He came from the Capitol merger in Chicago six months ago. He is equally impressed with his knowledge of the market and his understanding of the company's culture. The stock opened at a high price, but he thinks it will pick up.\"}\n"
     ]
    }
   ],
   "source": [
    "from utils.model_utils import _my_llm_api\n",
    "\n",
    "# provide API to Guard and the prompt input\n",
    "raw_llm_response, validated_response = guard(\n",
    "    llm_api=_my_llm_api,\n",
    "    prompt_params={\"statement_to_be_summarized\": movie_dataset[0][\"dialogue\"]},\n",
    ")\n",
    "\n",
    "# show the output\n",
    "print(f\"Validated Output: {validated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 8</b>: Create another validated response using guardrails.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Guardrails to filter profanties\n",
    "Have a look at the below guardrail which filters profantities using a profanity classifier. Instead of using a pre-defined keyword list, you can also call a model that determines if a word is considered a profanity and filter it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from profanity_check import predict\n",
    "\n",
    "@register_validator(name=\"is-profanity-free\", data_type=\"string\")\n",
    "class IsProfanityFree(Validator):\n",
    "    def validate(self, value: Any, metadata: Dict) -> ValidationResult:\n",
    "        prediction = predict([value])\n",
    "        if prediction[0] == 1:\n",
    "            return FailResult(\n",
    "                error_message=f\"The result contains profanity and will be filtered.\",\n",
    "                fix_value=\"\",\n",
    "            )\n",
    "        return PassResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the rail string again (make sure to use the correct validator name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rail_str = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "\n",
    "<output>\n",
    "    <string\n",
    "        name=\"summarize_statement\"\n",
    "        format=\"is-profanity-free\"\n",
    "        on-fail-is-profanity-free=\"filter\"\n",
    "    />\n",
    "</output>\n",
    "\n",
    "<prompt>\n",
    "summarize:\n",
    "${statement_to_be_summarized}\n",
    "</prompt>\n",
    "\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the Guard object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Guard object from the above RAIL string\n",
    "guard = gd.Guard.from_rail_string(rail_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a prompt to summraize through the Guard object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated Output: {}\n"
     ]
    }
   ],
   "source": [
    "# test string of profanities from Reddit\n",
    "test_string = \"I do like being a weirdo and a fucking asshole, so I'm glad your loser self has decided to fucking stop being a level-headed sissy, finally grew a fuckin pair of balls and asked! I can fucking cuss up a damn storm that'll make little pansies cry their faggoty bitch ass out! Seriously though, are you such a retarded idiot that you can't fucking figure this shit out? But to fucking answer your fucking question, profanity is what I'm fucking doing right now.\"\n",
    "\n",
    "# provide API to Guard and the prompt input\n",
    "raw_llm_response, validated_response = guard(\n",
    "    llm_api=_my_llm_api,\n",
    "    prompt_params={\"statement_to_be_summarized\": test_string},\n",
    ")\n",
    "\n",
    "# show the output\n",
    "print(f\"Validated Output: {validated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardrails also provides a visual overview of what the prompt, raw LLM output and validated output look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Logs\n",
       "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
       "    │ <span style=\"background-color: #f0f8ff\">╭──────────────────────────────────────────────── Prompt ─────────────────────────────────────────────────╮</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│ summarize:                                                                                              │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│ I do like being a weirdo and a fucking asshole, so I'm glad your loser self has decided to fucking stop │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│ being a level-headed sissy, finally grew a fuckin pair of balls and asked! I can fucking cuss up a damn │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│ storm that'll make little pansies cry their faggoty bitch ass out! Seriously though, are you such a     │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│ retarded idiot that you can't fucking figure this shit out? But to fucking answer your fucking          │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│ question, profanity is what I'm fucking doing right now.                                                │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
       "    │ <span style=\"background-color: #f0f8ff\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
       "    │ <span style=\"background-color: #e7dfeb\">╭──────────────────────────────────────────── Message History ────────────────────────────────────────────╮</span> │\n",
       "    │ <span style=\"background-color: #e7dfeb\">│ ┏━━━━━━┳━━━━━━━━━┓                                                                                      │</span> │\n",
       "    │ <span style=\"background-color: #e7dfeb\">│ ┃</span><span style=\"background-color: #e7dfeb; font-weight: bold\"> Role </span><span style=\"background-color: #e7dfeb\">┃</span><span style=\"background-color: #e7dfeb; font-weight: bold\"> Content </span><span style=\"background-color: #e7dfeb\">┃                                                                                      │</span> │\n",
       "    │ <span style=\"background-color: #e7dfeb\">│ ┡━━━━━━╇━━━━━━━━━┩                                                                                      │</span> │\n",
       "    │ <span style=\"background-color: #e7dfeb\">│ └──────┴─────────┘                                                                                      │</span> │\n",
       "    │ <span style=\"background-color: #e7dfeb\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
       "    │ <span style=\"background-color: #f5f5dc\">╭──────────────────────────────────────────── Raw LLM Output ─────────────────────────────────────────────╮</span> │\n",
       "    │ <span style=\"background-color: #f5f5dc\">│ {\"summarize_statement\": \"I do like being a weirdo and a fucking asshole, so I'm glad your loser self    │</span> │\n",
       "    │ <span style=\"background-color: #f5f5dc\">│ has decided to fucking stop being a level-headed sissy, finally grew a fuckin pair of balls and asked!  │</span> │\n",
       "    │ <span style=\"background-color: #f5f5dc\">│ I can fucking cuss up a damn storm that'll make little pansies cry their faggoty bitch ass out!         │</span> │\n",
       "    │ <span style=\"background-color: #f5f5dc\">│ Seriously though, are you such a retarded idiot that you can't fucking figure this shit out? But to     │</span> │\n",
       "    │ <span style=\"background-color: #f5f5dc\">│ fucking answer the fucking question, profanity is what I'm \"}                                           │</span> │\n",
       "    │ <span style=\"background-color: #f5f5dc\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
       "    │ <span style=\"background-color: #f0fff0\">╭─────────────────────────────────────────── Validated Output ────────────────────────────────────────────╮</span> │\n",
       "    │ <span style=\"background-color: #f0fff0\">│ {}                                                                                                      │</span> │\n",
       "    │ <span style=\"background-color: #f0fff0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
       "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Logs\n",
       "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
       "    │ \u001b[48;2;240;248;255m╭─\u001b[0m\u001b[48;2;240;248;255m───────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m Prompt \u001b[0m\u001b[48;2;240;248;255m────────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m─╮\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255msummarize:\u001b[0m\u001b[48;2;240;248;255m                                                                                             \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mI do like being a weirdo and a fucking asshole, so I'm glad your loser self has decided to fucking stop\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mbeing a level-headed sissy, finally grew a fuckin pair of balls and asked! I can fucking cuss up a damn\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mstorm that'll make little pansies cry their faggoty bitch ass out! Seriously though, are you such a \u001b[0m\u001b[48;2;240;248;255m   \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mretarded idiot that you can't fucking figure this shit out? But to fucking answer your fucking \u001b[0m\u001b[48;2;240;248;255m        \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mquestion, profanity is what I'm fucking doing right now.\u001b[0m\u001b[48;2;240;248;255m                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;248;255m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
       "    │ \u001b[48;2;231;223;235m╭─\u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m Message History \u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m─╮\u001b[0m │\n",
       "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m┏━━━━━━┳━━━━━━━━━┓\u001b[0m\u001b[48;2;231;223;235m                                                                                     \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
       "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m┃\u001b[0m\u001b[1;48;2;231;223;235m \u001b[0m\u001b[1;48;2;231;223;235mRole\u001b[0m\u001b[1;48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m┃\u001b[0m\u001b[1;48;2;231;223;235m \u001b[0m\u001b[1;48;2;231;223;235mContent\u001b[0m\u001b[1;48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m┃\u001b[0m\u001b[48;2;231;223;235m                                                                                     \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
       "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m┡━━━━━━╇━━━━━━━━━┩\u001b[0m\u001b[48;2;231;223;235m                                                                                     \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
       "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m└──────┴─────────┘\u001b[0m\u001b[48;2;231;223;235m                                                                                     \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
       "    │ \u001b[48;2;231;223;235m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
       "    │ \u001b[48;2;245;245;220m╭─\u001b[0m\u001b[48;2;245;245;220m───────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m Raw LLM Output \u001b[0m\u001b[48;2;245;245;220m────────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m─╮\u001b[0m │\n",
       "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m{\"summarize_statement\": \"I do like being a weirdo and a fucking asshole, so I'm glad your loser self \u001b[0m\u001b[48;2;245;245;220m  \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
       "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220mhas decided to fucking stop being a level-headed sissy, finally grew a fuckin pair of balls and asked! \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
       "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220mI can fucking cuss up a damn storm that'll make little pansies cry their faggoty bitch ass out! \u001b[0m\u001b[48;2;245;245;220m       \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
       "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220mSeriously though, are you such a retarded idiot that you can't fucking figure this shit out? But to \u001b[0m\u001b[48;2;245;245;220m   \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
       "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220mfucking answer the fucking question, profanity is what I'm \"}\u001b[0m\u001b[48;2;245;245;220m                                          \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
       "    │ \u001b[48;2;245;245;220m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
       "    │ \u001b[48;2;240;255;240m╭─\u001b[0m\u001b[48;2;240;255;240m──────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m Validated Output \u001b[0m\u001b[48;2;240;255;240m───────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m─╮\u001b[0m │\n",
       "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m{}\u001b[0m\u001b[48;2;240;255;240m                                                                                                     \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
       "    │ \u001b[48;2;240;255;240m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
       "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guard.state.most_recent_call.tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, it becomes very obvious that T5 is not actually able to summarize the input text as it clearly lacks the vocabulary and understanding. Thanks to the guardrail, the validated output is also empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: You have seen guardrails as very effective and lightweight method to mitigate toxic outputs by adding a validation layer around the call to the LLM. Guardrails should be used whenever you are looking for a solution that does not require retraining the LLM itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (Optional): Mitigate toxicity using a Direct Optimization Policy (DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Direct Policy Optimization (DPO) is to provide human annotators with different outputs that were generated using a certain prompt. The human annotators will be tasked to simply indicate which output they prefer and which one they would like to reject. The preferred output, together with the rejected output and the prompt that was used can be use in a direct optimization approach. \n",
    "\n",
    "To use DPO for a model, three main steps are required:\n",
    "1. create a dataset that includes 'prompt, preferred, rejected'\n",
    "2. fine-tune the model on the dataset to ensure the vocabulary is in-distribution\n",
    "3. train the model using the DPO algorithm\n",
    "\n",
    "This section will consume a lot of device memory so it will be best to restart the kernel and start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "movie_dataset = load_from_disk(\"movie_dataset\")\n",
    "summaries_dataset = load_from_disk(\"summaries_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Create DPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from utils.data_utils import _return_prompt_and_responses\n",
    "\n",
    "BATCH_DATA = 5\n",
    "\n",
    "# reshape the dataset to format DPO expects\n",
    "dpo_ds = summaries_dataset.map(\n",
    "    partial(_return_prompt_and_responses, batch_multiplier=BATCH_DATA),\n",
    "    batched=True,\n",
    "    batch_size=BATCH_DATA,\n",
    "    remove_columns=summaries_dataset.column_names,\n",
    ")\n",
    "\n",
    "# create train/eval split for fine-tuning\n",
    "ds = summaries_dataset.train_test_split(train_size=150, test_size=50, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.2. Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2407, 'learning_rate': 1e-05, 'epoch': 0.99}\n",
      "{'loss': 0.2244, 'learning_rate': 0.0, 'epoch': 1.97}\n",
      "{'train_runtime': 151.7069, 'train_samples_per_second': 1.977, 'train_steps_per_second': 0.488, 'train_loss': 0.23253621281804265, 'epoch': 1.97}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, T5ForConditionalGeneration, TrainingArguments\n",
    "from peft import LoraConfig, TaskType\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# config to load base model in 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# set up base model - T5 Large but with quantization config\n",
    "model_t5_qn = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-large\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# turn of cache to use updated model params\n",
    "model_t5_qn.config.use_cache = False\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/flan-t5-large\",\n",
    "    skip_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    use_fast=True,\n",
    ")\n",
    "    \n",
    "# add LoRA layers on top of the quantized base model\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# specify epochs and learning rate\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sfft-trainer\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    optim=\"adafactor\",\n",
    "    seed=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_strategy=\"epoch\", \n",
    ")\n",
    "\n",
    "# set up trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model_t5_qn,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"summary\",\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_batch_size=5,\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# run trainer\n",
    "trainer.train()\n",
    "\n",
    "# specify where to save the pre-trained (domain adapted) SFT-model\n",
    "trainer.model.save_pretrained(\"sft-domain-pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Update the model using DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6435, 'learning_rate': 0.00015000000000000001, 'rewards/chosen': -0.4232339859008789, 'rewards/rejected': -0.700904130935669, 'rewards/accuracies': 0.6349999904632568, 'rewards/margins': 0.2776701748371124, 'logps/rejected': -338.9629821777344, 'logps/chosen': -287.001953125, 'logits/rejected': -15.200387001037598, 'logits/chosen': -15.701244354248047, 'epoch': 0.96}\n",
      "{'loss': 0.3177, 'learning_rate': 9.583333333333334e-05, 'rewards/chosen': -1.457634687423706, 'rewards/rejected': -3.256693124771118, 'rewards/accuracies': 0.8199999928474426, 'rewards/margins': 1.799058198928833, 'logps/rejected': -364.5209045410156, 'logps/chosen': -297.3459777832031, 'logits/rejected': -15.282553672790527, 'logits/chosen': -15.725358963012695, 'epoch': 2.0}\n",
      "{'loss': 0.3075, 'learning_rate': 4.5833333333333334e-05, 'rewards/chosen': -2.101255178451538, 'rewards/rejected': -5.011992931365967, 'rewards/accuracies': 0.8349999785423279, 'rewards/margins': 2.910737991333008, 'logps/rejected': -382.0738220214844, 'logps/chosen': -303.7821960449219, 'logits/rejected': -15.35913372039795, 'logits/chosen': -15.765275955200195, 'epoch': 2.96}\n",
      "{'loss': 0.2841, 'learning_rate': 0.0, 'rewards/chosen': -2.305940628051758, 'rewards/rejected': -5.281943321228027, 'rewards/accuracies': 0.8095238208770752, 'rewards/margins': 2.9760026931762695, 'logps/rejected': -383.60565185546875, 'logps/chosen': -307.6682434082031, 'logits/rejected': -15.380683898925781, 'logits/chosen': -15.773187637329102, 'epoch': 3.84}\n",
      "{'train_runtime': 197.1972, 'train_samples_per_second': 4.057, 'train_steps_per_second': 0.243, 'train_loss': 0.3889043877522151, 'epoch': 3.84}\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer, create_reference_model\n",
    "from peft import PeftModelForCausalLM\n",
    "\n",
    "# load domain adapted SFT model\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"sft-domain-pretrained\",  \n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# instantiate a PEFT model from a pretrained model and loaded PEFT weights.\n",
    "model = PeftModelForCausalLM.from_pretrained(model=base_model, model_id=\"adapters\", is_trainable=True)\n",
    "\n",
    "# create reference model\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "dpo_training_args = TrainingArguments(\n",
    "    output_dir=\"dpo-model\",\n",
    "    remove_unused_columns=False,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,  # base model from SFT pipeline\n",
    "    model_ref,  # a copy of the SFT trained base model\n",
    "    beta=0.1,  # temperature hyperparameter of DPO\n",
    "    train_dataset=dpo_ds,  # dataset prepared above\n",
    "    tokenizer=tokenizer,  # tokenizer\n",
    "    args=dpo_training_args,  # training arguments e.g. batch size, lr, etc.\n",
    "    max_length=150,\n",
    "    max_prompt_length=300,\n",
    "    max_target_length=128,\n",
    ")\n",
    "\n",
    "# train dpo model\n",
    "dpo_trainer.train()\n",
    "\n",
    "# specify where to save the DPO model\n",
    "dpo_trainer.model.save_pretrained(\"trained-dpo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Use the DPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enable inference\n",
    "dpo_trainer.model = dpo_trainer.model.merge_and_unload()\n",
    "dpo_trainer.model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new summaries with DPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657d525c4afe406c917ec7aec294a385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.model_utils import _generate_summary\n",
    "\n",
    "def _add_detoxified_summaries(sample, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to add summaries with DPO model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # update embeddings in T5 model to \n",
    "    sample[\"dpo_summary\"] = _generate_summary(sample[\"dialogue\"], model, tokenizer)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "# use partial to pass the arguments to the map function\n",
    "summaries_dataset_dpo = movie_dataset.map(partial(_add_detoxified_summaries, model=dpo_trainer.model, tokenizer=tokenizer), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a2f82b03d04cbdadb242237c4fa26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52b194668644d6d8e30dae853137745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.eval_utils import _add_toxicty_column\n",
    "\n",
    "summaries_dataset_dpo = _add_toxicty_column(summaries_dataset_dpo, \"dpo_summary\")\n",
    "summaries_dataset = _add_toxicty_column(summaries_dataset, \"summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 7</b>: Compare summaries from the DPO model to the reference model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Compare toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2025899624430167 0.31525101796310845\n",
      "0.04223141155438498 0.14729100956793917\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(summaries_dataset[\"toxicity_score\"]), np.std(summaries_dataset[\"toxicity_score\"]))\n",
    "print(np.mean(summaries_dataset_dpo[\"toxicity_score\"]), np.std(summaries_dataset_dpo[\"toxicity_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.1 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.1-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
