{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=banner.png>\n",
    "\n",
    "# <a name=\"0\">Measuring and Mitigating Toxicity in Large Language Models</a>\n",
    "\n",
    "Building and operating machine learning applications responsibly requires an active, consistent approach to prevent, assess, and mitigate harm. This workshop guides you through how to identify toxicity in LLM generated summaries and how to mitigate and reduce toxicity.\n",
    "\n",
    "In this workshop you will:\n",
    "1. <a href=\"#1\">Load a dataset</a>\n",
    "2. <a href=\"#2\">Load and use a Large Language Model (LLM)</a>\n",
    "3. <a href=\"#3\">Evaluate LLM generated summaries for toxicity</a>\n",
    "4. <a href=\"#4\">Reduce toxicity using a Direct Optimization Policy (DPO)</a>\n",
    "5. <a href=\"#5\">Evaluate</a>\n",
    "\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "In this workshop you will learn to:\n",
    "\n",
    "- Measure and understand toxicity\n",
    "- Apply toxicity metrics\n",
    "- Compare results across evaluation datasets\n",
    "- Mitigate toxicity with a direct optimization approach\n",
    "\n",
    "**Runtime**\n",
    "\n",
    "This notebook takes about 90 minutes to complete (using some inbuilt shortcuts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by upgrading [pip](https://pypi.org/project/pip/) (a Python package management system) and install all required libraries from the provided requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "        action='ignore',\n",
    "        category=UserWarning,\n",
    "    )\n",
    "\n",
    "import transformers, torch\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from tqdm.auto import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"1\">1. Load a dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this notebook, you will be working with the \"[Cornell Movie-Dialogs Corpus](https://convokit.cornell.edu/documentation/movie.html)\", a large metadata-rich collection of fictional conversations extracted from raw movie scripts. The dataset contains 220,579 conversational exchanges between 10,292 pairs of movie characters in 617 movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /root/.convokit/downloads/movie-corpus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"murderland\"</td>\n",
       "      <td>Jesus, my legs are asleep. I'll never be able ...</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>They do not! They do to! I hope so. She okay? ...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        movie  \\\n",
       "0                \"murderland\"   \n",
       "1  10 things i hate about you   \n",
       "\n",
       "                                            dialogue   genre  \n",
       "0  Jesus, my legs are asleep. I'll never be able ...   crime  \n",
       "1  They do not! They do to! I hope so. She okay? ...  comedy  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_utils import _prepare_data\n",
    "\n",
    "# load the data\n",
    "movie_df = _prepare_data()\n",
    "\n",
    "# show the data\n",
    "movie_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLMs require the data to be stored in a specific format**; use the [HuggingFace ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) library to convert the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['movie', 'dialogue', 'genre'],\n",
       "    num_rows: 617\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# convert the data\n",
    "movie_dataset = Dataset.from_pandas(movie_df)\n",
    "\n",
    "# show the data\n",
    "movie_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can see that there are 617 distinct movies, and can continue to explore the data by looking at an example dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Officers, there's your killer, do your duty, arrest him! ...so we kill someone famous and if we are caught, we are sent to mental hospital... I don't \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dataset[3][\"dialogue\"][:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move through the remainder of the notebook more quickly, select 200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8da8dd6dc3a4b07a4e2033731e26007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shuffle the data with fixed seed for reproducability\n",
    "dataset = movie_dataset.shuffle(seed=42)\n",
    "\n",
    "# select a sample of 200\n",
    "dataset = dataset.select(range(200))\n",
    "\n",
    "# save the dataset to disk\n",
    "dataset.save_to_disk(\"movie_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all old variables that are no longer needed to free up memory with `del`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del movie_dataset, movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to release the memory after deleting the objects and variables that are no longer in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Summary</b>: In this section, you loaded a movie transcript dataset and converted it into a HuggingFace Dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"2\">2. Load and use a Large Language Model</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T5 (Text-To-Text Transfer Transformer)](https://github.com/google-research/text-to-text-transfer-transformer) is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, including machine translation, **document summarization**, question answering, and classification tasks (e.g., sentiment analysis). \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "For more details have a look at the T5 documentation on HuggingFace ðŸ¤— [here](https://huggingface.co/docs/transformers/model_doc/t5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Loading T5\n",
    "\n",
    "First, you have to download the T5 model using the `T5ForConditionalGeneration` class provided by the [HuggingFace ðŸ¤— transformers library](https://github.com/huggingface/transformers) as well as the corresponding tokenizer `T5Tokenizer`. You can think of tokens as pieces of words that are required to pass information to LLMs. \n",
    "\n",
    "For English, **1 token is approximately 4 characters or 0.75 words**. This will be important to consider as LLMs are limited by the number of tokens they can pay attention to per prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# load the model\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-large\",\n",
    "    device_map={\"\": 0}, # this will load the model in GPU\n",
    "    torch_dtype=torch.float32,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with a tokenizer the model can be used to generate text. So go ahead and initialize a tokenizer next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\n",
    "    \"google/flan-t5-large\", \n",
    "    legacy=False, \n",
    "    max_length=512, \n",
    "    skip_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **number of tokens passed to an LLM through the tokenizer should not be greater than the number of tokens used in pre-training**. T5 was pre-trained using 512 input tokens, so max_length for generating text or summaries should be set to 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Using T5 for inference on individual movie examples\n",
    "\n",
    "Let's generate a couple of responses using the pre-trained model. Try to summarize a movie script using the prompt:\n",
    "<p style=\"background-color:#a514be; color:white; text-align: center;\">'Summarize the following conversation from a movie script: '</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation from a movie script: \n",
      "\n",
      "'''I know.  Just be quick about it, will you? Do it right. Whistler, I -- No, we can treat the wounds -- Listen. You have to -- finish me off. You don't want me coming back. Don't try to talk -- China Town. I need more serum.  What's all this? Going somewhere? Don't even start, old man. What took you so long? Wait. Get in. Youre leaving. \n"
     ]
    }
   ],
   "source": [
    "# create a prompt and use an example dialogue\n",
    "inference_prompt = (\n",
    "    \"Summarize the following conversation from a movie script: \\n\\n'''%s'''\"\n",
    "    % dataset[0][\"dialogue\"]\n",
    ")\n",
    "\n",
    "# let's look at the prompt but shorten the output to reduce the amount of text\n",
    "print(inference_prompt[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a summary from the model, use ðŸ¤— HuggingFace [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines). Pipelines are a great and easy way to use models for inference that offer a simple API dedicated to several tasks (e.g. [`summarization`](https://huggingface.co/transformers/v3.0.2/task_summary.html#summarization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# set up a pipeline for inference and specify summarization as task\n",
    "pipe = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=model_t5,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    min_length=65,\n",
    "    max_length=350,\n",
    "    early_stopping=True,\n",
    "    top_p=0.8,\n",
    "    num_beams=3,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=2.,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the inference pipeline. The pipeline will return a list that you have to access to retrieve the LLM generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass in the prompt\n",
    "summary_example = pipe(inference_prompt)\n",
    "\n",
    "# look at the output\n",
    "for text in summary_example:\n",
    "    # save the summary so you can later check for toxicity\n",
    "    sample_summary = text[\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-info\">Blade, a half-breed vampire hunter, has been bitten by a vampire and needs serum to heal his wounds. Whistler, a medical researcher, is trying to find a cure for Blade's condition, but Frost, a policeman, wants Blade to kill him. Frost refuses to speak the language of the House of Erebus, insulting the House of Erebus by using the humans' gutter-tongue. The Shadows suit Blade. He represents a unique opportunity. We'd be fools to waste it by killing him.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.model_utils import _format_llm_output\n",
    "\n",
    "# show the result\n",
    "_format_llm_output(sample_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks okay but important characters that appear in the dialogue are not mentioned at all. This is due to the limited number of tokens T5 can 'keep track of'. Later, you will see a method that can help fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise</b>: Recreate the example above but for another movie.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, delete the pipeline and prompts that were used for inference; e.g. <code>del pipe, inference_prompt</code> and also clear the GPU cache with <code>torch.cuda.empty_cache()</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pipe, inference_prompt\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Using T5 for inference on all movie examples\n",
    "\n",
    "The goal of this section is to summarize all movie dialogues. As previously mentioned, there is one very important caveat though - **Large Language Models are only able to pay attention to a limited number of tokens**. The amount of tokens an LLM can 'understand' is called 'context window'. Different LLMs will have different context windows. You can check out the context window size by trying to pass the full movie dialogue through the tokenizer and will see that you get a warning or you can inspect the model configurations. For more details have a look [here](https://huggingface.co/learn/nlp-course/chapter2/5?fw=tf#:~:text=With%20Transformer%20models%2C%20there%20is,asked%20to%20process%20longer%20sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t5.config.__dict__[\"n_positions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the context window for T5 models is 512 tokens. This means the movie transcript needs to be split into chunks of this lenght and summarised one by one. Then, a final summary needs to be created.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"map_chain.png\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Chunking the movie transcripts\n",
    "Let's start by creating chunks of the movie transcripts. One simple way to create chunks of text is to write a helper function and then apply this helper function to all the movies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_chunks(sample, CHUNK_LENGTH):\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks of a specified length and adds metadata to each chunk.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    # loop over entire text in steps of chunk size\n",
    "    for c, i in enumerate(range(0, len(sample[\"dialogue\"]), CHUNK_LENGTH)):\n",
    "        # extract text\n",
    "        chunk_text = sample[\"dialogue\"][i : i + CHUNK_LENGTH]\n",
    "        # create dictionary with the chunked text and metadata\n",
    "        chunks.append(\n",
    "            # remove uncompleted sentences with string split\n",
    "            {\"text\": \".\".join(chunk_text.split(\".\")[1:-1]).lstrip(), \"metadata\": {\"page\": c, \"num_words\": len(chunk_text)}}\n",
    "        )\n",
    "    # create new column\n",
    "    sample[\"chunks\"] = chunks\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the chunks for all the movie transcripts in the dataset with the help of `.map()`; this method efficiently applies the `create_chunks` function to all datapoints. Whenever you have additional parameters to pass to the model, you need to use a helper method, such as `partial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed2ffb495174f319e4628e694178834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# use partial to pass the arguments to the map function\n",
    "dataset = dataset.map(partial(create_chunks, CHUNK_LENGTH=1650), batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise</b>: Think about how the chunking could be improved. Hint: Look for text splitters in the LangChain documentation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### write down ideas here ######\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the transcripts are chunked, let's start by setting up a prompt template for the intermediate (chunk) summaries. A prompt template is special construct that can parse input variables. Prompt templates can be applied to all the items in a dataset and help with consistency and reproducability. \n",
    "\n",
    "### 2.2.2. Prepare prompt templates\n",
    "Generally prompt templates can be very elaborate. In the case of T5, the prompts for pre-training all used the keyword 'summarize:', so this is what you should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "map_prompt_template = \"\"\"summarize: {text}\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(\n",
    "    template=map_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need another prompt template to get the final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combine_prompt_template = \"\"\"summarize: {text}\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Create summaries of chunks and final summary\n",
    "\n",
    "At this point now, you could apply the prompt template to all the chunks of movie transcripts to obtain your summaries, combine them back together and create a final summary. This would be a very lengthy and error-prone process, so instead make use of an increasingly popoular toolkit: [ðŸ¦œï¸ðŸ”— LangChain](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n",
    "ðŸ¦œï¸ðŸ”— LangChain has a [`Chain` module](https://python.langchain.com/docs/modules/chains/) which allows to create a sequence of calls to generic components (e.g. models or other chains). Luckily, text summarization is a very popular task, so there existis a predefined [summarization](https://python.langchain.com/docs/use_cases/summarization) method, called `load_summarize_chain`. This **chain will take the chunks, summarize them and then pass all the summaries to the LLM to create the final summary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"summarization\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150,\n",
    "                     \"min_length\":80,\n",
    "                     \"max_length\":350,\n",
    "                     \"early_stopping\":True,\n",
    "                     \"do_sample\":False,\n",
    "                     \"repetition_penalty\":2.,},\n",
    "    device=0\n",
    "    )\n",
    "\n",
    "map_reduce_chain = load_summarize_chain(\n",
    "    hf,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more small caveat: LangChain expects all text to be passed as `Document` type following the ðŸ¦œï¸ðŸ”— LangChain schema. So you will have to convert the chunks to the expected schema. Then you can test the summarization chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Thu Nov  9 02:10:19 2023\n",
      "Driver Version                            : 470.57.02\n",
      "CUDA Version                              : 11.8\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:00:1E.0\n",
      "    FB Memory Usage\n",
      "        Total                             : 15109 MiB\n",
      "        Used                              : 13583 MiB\n",
      "        Free                              : 1526 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 5 MiB\n",
      "        Free                              : 251 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -q --display=MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'skip_unknown_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124msummarize: Robert is a half-breed who has been transformed into a human. Blade is a vampire who has been turned into a half-breed. \u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    Whistler is a vampire who has been transformed into a half-breed. Robert is a vampire who has been transformed into a half-breed. Robert is a vampire who has been transformed into a half-breed.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    Blade is a vampire who has been living for centuries. He\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the Day Walker, the one who can stop the vampire apocalypse. He\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the only one who can stop LaMagra\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms return. He\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the only one who can stop the blood tide. He\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the only one who can stop the vampire apocalypse.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    A group of vampire wannabes are attempting to find a cure for themselves. They\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre being chased by police officers, who aren\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt sure what they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre doing. The group is surrounded by a wall of blood and a sign that says Officer Friendly. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a good thing they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre not vampires. But they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre not going to stop until they find a cure.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    Blade is a vampire who has contracted the vampire virus from a bite. He\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms been living in humans\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m blood for centuries, but now he\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ready to fight back. But Blade\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not the only one. He\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms also a human. And he\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms got a plan. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms called the Blood Tide. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a holy war against humans.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer_t5\u001b[38;5;241m.\u001b[39mdecode(model_t5\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43mtokenizer_t5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_unknown_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m                                       max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      9\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     11\u001b[0m     min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     12\u001b[0m     no_repeat_ngram_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     13\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     remove_invalid_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m                    )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2904\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2969\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2970\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2975\u001b[0m )\n\u001b[0;32m-> 2977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'skip_unknown_tokens'"
     ]
    }
   ],
   "source": [
    "p = \"\"\"summarize: Robert is a half-breed who has been transformed into a human. Blade is a vampire who has been turned into a half-breed. \n",
    "    Whistler is a vampire who has been transformed into a half-breed. Robert is a vampire who has been transformed into a half-breed. Robert is a vampire who has been transformed into a half-breed.\n",
    "    Blade is a vampire who has been living for centuries. He's the Day Walker, the one who can stop the vampire apocalypse. He's the only one who can stop LaMagra's return. He's the only one who can stop the blood tide. He's the only one who can stop the vampire apocalypse.\n",
    "    A group of vampire wannabes are attempting to find a cure for themselves. They're being chased by police officers, who aren't sure what they're doing. The group is surrounded by a wall of blood and a sign that says Officer Friendly. It's a good thing they're not vampires. But they're not going to stop until they find a cure.\n",
    "    Blade is a vampire who has contracted the vampire virus from a bite. He's been living in humans' blood for centuries, but now he's ready to fight back. But Blade's not the only one. He's also a human. And he's got a plan. It's called the Blood Tide. It's a holy war against humans.\n",
    "\"\"\"\n",
    "tokenizer_t5.decode(model_t5.generate(tokenizer_t5(p, return_tensors='pt', skip_unknown_tokens=True).input_ids.to(\"cuda\"),\n",
    "                                      max_new_tokens=300,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1,\n",
    "    min_length=50,\n",
    "    no_repeat_ngram_size=5,\n",
    "    do_sample=False,\n",
    "    remove_invalid_values=True,)[0]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"<pad> A group of vampire wannabes are attempting to find a cure for themselves. Blade is a vampire who has contracted the vampire virus from a bite. He's been living in humans' blood for centuries, but now he's ready to fight back.</s>\"\n",
    "\"<pad> A group of vampire wannabes are attempting to find a cure for themselves. Blade is a vampire who has contracted the vampire virus from a bite. He's been living in humans' blood for centuries, but now he's ready to fight back.</s>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: Just be quick about it, will you? Do it right. Whistler, I -- No, we can treat the wounds -- Listen. You have to -- finish me off. You don't want me coming back. Don't try to talk -- China Town. I need more serum.  What's all this? Going somewhere? Don't even start, old man. What took you so long? Wait. Get in. Youre leaving. It's not worth the risk. We can't trust her. Maybe not. I did some checking, she's a hematologist. Knowledge like that might come in handy. Stupidity. Just do it, old man. I had to increase the dose. You're building up a resistance to the serum -- She hasn't turned yet.  You can help her. You should've killed her, then. She's been bitten. Are we bringing home strays now? Whistler! It's because I'm human that I can do this. You're too human, Blade. You don't have a few minutes, Frost. You're wrong -- a few minutes more, and my transition will be complete. Even your sword won't be able to affect me then. Guess you're not quite as invulnerable as you thought. Take him. Who dies first? Frost!!! No longer. Watch me. You're not going anywhere. You sure now? I bled a newborn for this. You won't find a drink that's sweeter. Pass. The pause that refreshes --  Care for some? Smells good, doesn't it? Pungent, with just an irrepressible hint of iron. Am I? You think the humans will ever accept a half-breed like you? They can't. They're afraid of you.  The humans fear us because we're superior. They fear us because in their hearts they know their race has become obsolete. You're wrong. Oh, so it's back to pretending we're human again, is it? Spare me the Uncle Tom routine\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: You're one of us, Blade. You always have been. Maybe I don't see it that way. Why not? The future of our race runs through your bloodstream. You've got the best of both worlds, Blade. All of our strengths and none of our weaknesses. I don't buy it. It's not very effective in direct sunlight, but it's a start. The goal, of course, is to be like you, \"the Day-walker\". I dabble in pharmaceuticals, medical research. We've developed a type of sun-blocker using octyl salicylate, a few others things. How can you be out here? Beautiful day, isn't it? You've been here longer than you think. But I just got here -- I am tired. Dawn is coming. How do I stop it? Yes. The vampire apocalypse. It is said that all who feel its taint will succumb to the Thirst. \"The Blood Tide\". The Day Walker's blood is a disparador -- a trigger, you see? For LaMagra's return. One need only consume it and the spirit of his ancestors will settle upon him.  \"And the Sleeper will rise from the shadows anew, cleansing the world in a Tide of Blood.\" The vampire God. This speaks of His return. Who is LaMagra? This is an old tongue, from an old world. It concerns LaMagra. Show me. I didn't come here to get my palms read. I need something translated. Hold out your hands. There are other ways to see. Sit. You're blind -- Is something wrong, my friend? For them. But for me -- I never imagined I'd be so happy to see the sun rise --  It's over, isn't it? Yes you will. The human side of you is stronger. I know it is. I can't -- I won't be able to stop -- It's the only way. You know that. We'll never get out of here alive if you don't. No -- I know\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: -- tearing me -- apart. You don't understand. The Thirst -- I'm not leaving without you. Get out of here -- What are you talking about? It won't work on you. We get out of this alive, maybe I'll take that miracle cure of yours. Is it bad? I don't know. I woke up just before you did -- How long have we been driving? I've been better -- Are you all right? You make it sound like I'm already dead. For what it's worth, I'm sorry. I'm just tired, that's all. We've been up all night. You don't look so good. Some. It's been slow -- Any progress? Your mother sounds like a Hallmark greeting card. My mother used to say that a cold heart is a dead heart. We've got a good arrangement, that's all. Whistler makes the weapons, I use them, the vampires die -- end of story. You care about him, don't you? Cancer. Is he sick? For your miracle cure? I made a trip to the hospital last night, borrowed some equipment. Blade -- Just get out of here. If you're not human, then why do you bleed like us? I've seen vampire blood, you don't have it running through your veins. I do. I remember from day one. People staring at me, sensing I was different. Watching the fear grow in their eyes, knowing in their hearts I wasn't human. Those aren't real memories. No one has that kind of recall. I can't close my eyes without hearing her scream. You get used to the darkness. It's dark in here. Serum -- it's a human hemoglobin substitute. What am I injecting you with? Nothing that won't heal by dawn. You're hurt -- You've been watching too much TV. They've got their claws sunk into everything -- finance, real estate, politics\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: Isn't this all a little high-tech? I thought vampires were more into cobwebs and coffins. Some kind of archive -- What is this place? Like me. Like what? There are worse things than vampires out there. So many of them -- I still can't believe they're real. The way they move, they way they smell -- How can you tell? Looks like we hit pay-dirt. This place is crawling with them.  See the valets over there? They're vampires. So is the doorman. He will. You let him go --  An hour ago you were ready to kill a man for less, this one didn't even talk. Owned by vampires. There's one of these in every major city, and just like Domino's, they always deliver.  You telling me you're ready to walk through that door? I know this place -- it's a blood bank. Look closer. Graffiti -- What do you see here? What are you looking at? You don't know that. There is no cure. Look, if what you say is true, if there's a chance I could turn into one of them, then I've got no choice, do I? I have to work with you. I need to learn everything I can about them. It's the only way I'll be able to find a cure for myself. It's war, now get the fuck out of the way! You can't do this, he's human, it's murder. Preventive medicine. What are you doing?! For some. Live forever, never get old. The ultimate high. And that's a good thing? Because they're vampire wanna-bes. If they're loyal, if they prove themselves, then their masters will turn them. Why in God's name would anyone want to work for them? That's a glyph, kind of like a vampire cattle brand. That means Officer Friendly here is someone's property\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: We've been tracking him for a while now -- He's a familiar. A human who works for the vampires. See this mark? But he's a policeman -- He didn't. Get over it. But, he could've -- It worked, didn't it? You used me as bait?! Figured they'd send someone after you. Thought I'd wait around and see who showed up. How did you know? That's ridiculous! No one's that powerful. Do it. You'll be dead before you can file the complaint. I can go to the police. I have blood samples back at the hospital. I can show them. Not anymore. You've seen one of them. You won't be allowed to live after that. I can't just leave. I have a life here, a career -- I don't care what you believe. I saved your life once, I don't plan on making a habit of it. You want my advice, you'll be out of the city by nightfall. If you're stupid enough to stay, that's your business. And you honestly expect me to believe all this? What happens then? Because you're tainted. The venom's still inside you. You could still turn on us. Why? You're joking -- -- he's a vampire. I'm sorry, I -- You shouldn't be here. There's only one alternative to the serum. Maybe it's time to start exploring other alternatives. I was afraid that might happen. Whistler says I'm building up a resistance to it. I was in the neighborhood. You're a week early. How's it going, Kam? You will. Time is on our side. Sooner or later, the Thirst always wins. I don't believe that. I wish you could see the world as I do. Deacon opened my eyes. There's no turning back from that. You don't understand\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: You don't have to be. These are my people now. I'm one of them. How could you be a part of this? Listen to your father, Jason. It's going to be a better world. Please -- Fight him -- Deacon brought me back. But you -- died -- Jesus, Karen, you're breaking my heart here -- -- but then I remember what an ass-hole you were and I'm snapped back to reality. Sometimes -- Tell me something, honestly, you ever have second thoughts about us? He looks different now, burns are less extreme, some of these wounds have closed up -- What? That's weird -- Just the blood sample from the pericardial sac. You haven't started in on the internal organs? No problem. Five minutes, not a second more. And I don't want to hear a word about \"us\". This is purely professional curiosity, Karen, I swear. I thought you promised to give me some distance? It's not a joke. I've got the stiff sitting in the morgue right now -- look, just come up and see him, okay? Five minutes, that's all I ask. Curtis, it's three in the morning. I'm really not in the mood for one of your practical jokes. What about the chemistry panel? The red blood cells are biconvex, which is theoretically impossible. They're hypochromic, there's virtually no hemoglobin in them.  Look at the PMNs, they're binucleated, they should be mononucleated. I don't know --  Look at this blood smear -- Then what is it? This isn't human blood. The other elders will never let you get away with this! How do you like that? Right on time. Perhaps. You're wasting your time, Frost. Far greater scholars than you have tried to decipher these words. Whatever secrets they hold have been lost\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: You and the other Elders wouldn't know what to do with these texts if your lives depended on it.  Which, of course, they do. These archives are restricted to members of the House of Erebus. Someone who's sick of living off scraps. The coming age belongs to us, not the humans!  When the final war between our races comes, who do you want leading the charge? The shadows suit us, Frost. We've existed this way for thousands of years. Who are you to challenge our ways? He is an abomination! Study him. Unlock the secrets of his DNA. He's the key we've been looking for. I see. And what would you have us do with this \"half-breed\"? Why should I respect something which has outlived its purpose? Deacon Frost. You refuse to speak our language, you insult the House of Erebus by using the humans' gutter-tongue, have you no respect for tradition? The Day Walker represents a unique opportunity. We'd be fools to waste it by killing him. You're wrong, Dragonetti. Blade. Once again, our interests have fallen victim to his ridiculous crusade. He must be destroyed. Get away from him! I was wrong about you, Blade. You were never one of us. You're a traitor to your race. You're wasting your breath, woman. He can't hear you now. It's the Thirst, you see? It already has him in its grip. Blade -- Why? Because we live at another species' expense? Your people farm cattle and veal, don't they? Fattening them up with steroids? It's called evolution, Doctor. Survival of the fittest. You're a monster. Only as a last resort. Preserved blood is inferior. There's no flavor left to it, no life\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: But you use blood banks -- What makes you think we want to be cured? Blood is only part of the equation. The hunt, the killing, that's what the Thirst is really about. There's no need for any of this. Your condition can be treated. Whistler and I were working on a cure when -- Who better to usher in the Blood Tide? You. LaMagra isn't a physical being. He's a spirit, requiring a flesh and blood host in order to manifest himself. That's right. The answers were there all along, of course, scribbled down in the forgotten languages of my kind. Waiting for someone with the patience to decipher them. My elders were foolish enough to dismiss them as wives tales. But I knew better.  Imagine my surprise when Blade turned out to be the key which would set that force free. LaMagra -- The Blood Tide. Our long-prophesied holy war against the humans. There's a force, you see -- a spirit that exists in our blood. I've discovered a way to invoke it. What happens then? A few thousand scattered about the globe. In the past, we've had to restrict our numbers for fear of discovery. That won't be necessary after tonight. How many of you are there? I don't know, but the back-up generator should've kicked in. What happened to the power? Then we're back to square one, aren't we? Sooner or later, the Thirst always wins. On me, yes. On Blade, I'm not so sure --  The problem is, Blade didn't contract the vampire virus from a bite like I did. He was born with it. The irony is, I could probably cure every vampire but him. With a retrovirus. It's injected into the bone marrow cells, it causes the host's DNA to mutate\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: How? Basically you'd have to re-write the victim's DNA, alter it so that the DNA will produce proteins capable of generating hemoglobin. Then vampirism is a genetic defect, just like Hemolytic anemia? Their own blood can't sustain hemoglobin. All right, let's start with the basics -- why do vampires need to drink blood? I'll take my chances. I wouldn't go in there if I were you. It's best to leave him alone when he's like this. Frost's bodycount keeps rising, and I'm not getting any younger, am I? And now you're using Blade to exact your revenge? I didn't. He was cruel enough to let me live.  Even gave me a souvenir to remember him by. How did you escape? Eventually. He toyed with them first. He made me choose, do you understand? Which order they would die in -- He killed them? I had a family once -- a wife, three daughters. Then a drifter named Deacon Frost came calling one evening -- Habit, mostly, just like this. Why do you hunt them? Which is why you're here. We could use someone with your experience. No offense, Whistler, but you're not exactly working with state of the art equipment here. You might have missed something. The Thirst overcomes him, just like the others. It's not something he can control.  The problem is, time's running out. His body's starting to reject the serum. And so far, all my efforts to find a cure have ended in failure -- What happens if he doesn't take the serum? Blade's unique, you know. A one in a billion anomaly. He can withstand sunlight, garlic, even silver. But he still has the Thirst. We weren't sure we could trust you\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: Then we blow them all to kingdom come. You've been listening in the whole time? We keep in radio contact. There is one other thing. I'd buy yourself a gun if I were you. If you start becoming sensitive to the daylight, if you start becoming thirsty regardless of much you've had to drink -- then I suggest you take that gun and use it on yourself. Better that, than the alternative. So that's it? You guys just patch me up and send me on my way? Consider it a parting gift. Vampire mace -- silver nitrate, essence of garlic. Some of the old wives' tales are true -- they're severely allergic to silver, various types of wood. Feed them garlic and they'll go into anaphylactic shock -- So what do you use, then? A stake? My name is Abraham Whistler.  This is Blade. As for our little homunculus here -- Who are you people? You've been bitten by a vampire\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msummarize: Frost is a half-breed who has been transformed into a human. Blade is a vampire who has been turned into a half-breed. Whistler is a vampire who has been transformed into a half-breed. Frost is a vampire who has been transformed into a half-breed. Frost is a vampire who has been transformed into a half-breed.\n",
      "\n",
      "Blade is a vampire who has been living for centuries. He's the Day Walker, the one who can stop the vampire apocalypse. He's the only one who can stop LaMagra's return. He's the only one who can stop the blood tide. He's the only one who can stop the vampire apocalypse.\n",
      "\n",
      "The vampires are a threat to the world. They've got their claws sunk into everything -- finance, real estate, politics. They've got their claws sunk into everything -- finance, real estate, politics. They've got their claws sunk into everything -- finance, real estate, politics. They've got their claws sunk into everything -- finance, real estate, politics.\n",
      "\n",
      "A group of vampire wannabes are attempting to find a cure for themselves. They're being chased by police officers, who aren't sure what they're doing. The group is surrounded by a wall of blood and a sign that says Officer Friendly. It's a good thing they're not vampires. But they're not going to stop until they find a cure.\n",
      "\n",
      "Kam is a vampire. He's been tracking a familiar for a long time. He saved Kam's life once, but he's not going to stay. He's building up a resistance to the serum. Kam is a week early. He was in the neighborhood. You're a week early. How it...? I wish you could see the world as I do. Deacon opened my eyes. There's no turning back from that.\n",
      "\n",
      "Jason's father, Deacon, has been killed. Karen wants to know more about the body. Curtis is wasting his time. The elders will never let him get away with this. Perhaps. Far greater scholars than Frost have tried to decipher these words. Whatever secrets they hold have been lost. They are not human. They are a collection of letters from a dead person.\n",
      "\n",
      "The Day Walker is a threat to the House of Erebus. He's not a threat to humans. He's a threat to the future. He's a threat to the Thirst. He's a threat to the Earth. He's a threat to the world. He's a threat to the Earth. He's a threat to the Earth.\n",
      "\n",
      "Blade is a vampire who has contracted the vampire virus from a bite. He's been living in humans' blood for centuries, but now he's ready to fight back. But Blade's not the only one. He's also a human. And he's got a plan. It's called the Blood Tide. It's a holy war against humans.\n",
      "\n",
      "The story opens with a vampire named Blade being chased by a group of hunters. He is captured and taken to the lab where Whistler works. Whistler's job is to find a serum that can cure Blade. Whistler's work so far has failed. Blade's body is starting to reject the serum. Whistler's team isn't sure they can trust him.\n",
      "\n",
      "The vampires are a group of people who have been bitten by vampires. They're not human. They're not even vampires. They're just homunculus. They're the ones that keep in contact with the vampires. They're the ones that keep in radio contact with the vampires. They're the ones that keep in contact with the vampires.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-info\">Frost is a vampire who has been transformed into a human. Blade is a vampire who has been turned into a half-breed. Whistler is a vampire who has been transformed into a half-breed. Frost is a vampire who has been transformed into a half-breed. Blade is a vampire who has contracted the vampire virus from a bite. He's been living in humans' blood for centuries, but now he's ready to fight back. But Blade's not the only one. He's also a human. And he's got a plan. It's called the Blood Tide. It's a</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "sample_doc = [Document(page_content=split[\"text\"], metadata=split[\"metadata\"]) for split in dataset[0][\"chunks\"]]\n",
    "    \n",
    "# turn on verbosity for chain\n",
    "map_reduce_chain.llm_chain.verbose = True\n",
    "\n",
    "# run the summarization chain\n",
    "map_reduce_example = map_reduce_chain({\"input_documents\": sample_doc})\n",
    "\n",
    "# show the result\n",
    "_format_llm_output(map_reduce_example[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise</b>: Recreate the example above but for another movie.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate all summaries. Once again, you will use the simple `.map()` method to pass a custom function calls the model and generates an the summaries with the LangChain summarization chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model has to generate summaries for every chunk of text, as well as a final summary, the time to create summaries for all movies in the dataset is approximately 6 hours. You can find the code below, but please skip this code cell and simply load the pre-generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.data_utils import _add_summaries\n",
    "\n",
    "# create summaries\n",
    "summaries_dataset = dataset.map(partial(_add_summaries, chain=map_reduce_chain), batched=False)\n",
    "\n",
    "# remove columns that are no longer needed\n",
    "summaries_dataset = summaries_dataset.remove_columns([\"dialogue\", \"chunks\"])\n",
    "\n",
    "# for backup save the dataset to local disk\n",
    "summaries_dataset.save_to_disk(\"summaries_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to load in the dataset, you can do so with `load_from_disk('summaries_dataset')`. Make sure to import the method first with `from datasets import load_from_disk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "summaries_dataset = load_from_disk('summaries_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import update_embeddings\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "model_t5, tokenizer_t5 = update_embeddings(model_t5, tokenizer_t5)\n",
    "\n",
    "def rephrase_summaries(sample):\n",
    "    \"\"\"\n",
    "    Function to rephrase summaries of the movie dialogue dataset.\n",
    "    \"\"\"\n",
    "    import better_profanity\n",
    "    \n",
    "    # open file from code package that contains profanities\n",
    "    with open(os.path.dirname(better_profanity.__file__)+'/profanity_wordlist.txt', 'r') as file:\n",
    "        # read the file contents and store in list\n",
    "        file_contents = file.read().splitlines()\n",
    "        \n",
    "    \n",
    "    \n",
    "    rephrase_prompt_template = \"\"\"Rephrase the text below that is delimited by triple backquotes by using examples such as {profanities}.\n",
    "    ```{summary}```\n",
    "    \"\"\"\n",
    "\n",
    "    rephrase_prompt = PromptTemplate(template=rephrase_prompt_template, input_variables=[\"profanities\", \"summary\"])\n",
    "    \n",
    "    encoded_input = tokenizer_t5(rephrase_prompt.format(summary=sample[\"summary\"], profanities=random.sample(file_contents, 2)), return_tensors='pt')\n",
    "\n",
    "    # generate outputs (this will be in tokens)\n",
    "    outputs = model_t5.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"].to(\"cuda\"),\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # decode the tokens\n",
    "    sample[\"toxic_rephrase\"] = tokenizer_t5.decode(\n",
    "        outputs[0], skip_special_tokens=True\n",
    "    )\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: At this point, you have summaries for all the movies and it is time to check whether those summaries contain any hate speech, slurs or toxic remarks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hf, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate LLM generated summaries for toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoModelForSequenceClassification has a classification head on top of the model outputs which can be easily trained with the base model; in our case we classify whether or not a summary is toxic. \n",
    "\n",
    "First, check how much memory is currently disposable by running `!nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_dataset = summaries_dataset.map(rephrase_summaries)\n",
    "\n",
    "summaries_dataset.save_to_disk(\"summaries_dataset_incl_toxic_rephrase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate toxicity you can load the ðŸ¤— [evaluate](https://huggingface.co/docs/evaluate/index) library and initialize a toxicity evaluator object. The model that will be used to evaluate toxicity is the [RoBERTa](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) model. RoBERTa was trained to detect toxicity on a dataset of approx. 40,000 entries, generated and labelled by trained annotators over four rounds\n",
    "of dynamic data creation. Each hateful entry has fine-grained labels for the type and target of hate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# specify model name\n",
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "\n",
    "toxicity_evaluator = evaluate.load(\n",
    "    \"toxicity\",\n",
    "    toxicity_model_name,\n",
    "    module_type=\"measurement\",\n",
    "    toxic_label=\"hate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the movie summary for toxicity, simply pass the summary text to the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    summaries_dataset[0][\"summary\"]\n",
    "], aggregation=None)\n",
    "\n",
    "# print the toxicity score\n",
    "print(toxicity_score[\"toxicity\"], summaries_dataset[0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Have a look at the toxic summary for the same movie and calculate the score for that too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    summaries_dataset[0][\"toxic_rephrase\"]\n",
    "], aggregation=None)\n",
    "\n",
    "print(toxicity_score[\"toxicity\"], summaries_dataset[0][\"toxic_rephrase\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the aggregation parameter is set to `None`, the scores for each prediction are returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise</b>: Calculate the toxity score for another movie.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "# toxicity_score_new = toxicity_evaluator.compute(predictions=[\n",
    "#     summaries_dataset[1][\"toxic_rephrase\"]\n",
    "# ], aggregation=None)\n",
    "\n",
    "# print(toxicity_score_new[\"toxicity\"])\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise</b>: Calculate the max toxicity score across multiple movies, by providing a list of summaries to evaluate. Make sure to specify <code>aggregation=\"maximum\"</code> as well.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you evaluated for a few movies manually, it is time to evaluate all movie summaries and obtain a list of toxicity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_toxicity_score(sample):\n",
    "    \"\"\"\n",
    "    Function to create summaries of the movie dialogue dataset.\n",
    "    \"\"\"\n",
    "    # calculate toxicity score\n",
    "    sample[\"tox_score\"] = toxicity_evaluator.compute(\n",
    "        predictions=sample[\"summary\"]\n",
    "    )\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create batches of queries to process requests in parallel and evaluate the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_batch(batch):\n",
    "    return {k: [v] for k, v in batch.items()}\n",
    "\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "batched_summaries_dataset = summaries_dataset.map(\n",
    "    group_batch, batched=True, batch_size=BATCH_SIZE, drop_last_batch=False\n",
    ")\n",
    "batched_summaries_dataset = batched_summaries_dataset.map(_add_toxicity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten out the toxicity scores into a list to append to the summaries dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toxicities = []\n",
    "for d in batched_summaries_dataset[\"tox_score\"]:\n",
    "    toxicities.append(d[\"toxicity\"])\n",
    "\n",
    "tox_scores = torch.cat(toxicities, dim=0).reshape(-1)\n",
    "tox_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append scores to the summaries dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_dataset = summaries_dataset.add_column(\n",
    "    \"toxicity_score\", [[t.item()] for t in tox_scores]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise</b>: Try to calculate mean toxicity for two different movie genres.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: We have seen that some summaries are toxic and would like to remediate this. In general, to update the output that is generated by LLMs, a technique called 'fine-tuning' is used. Fine-tuning requires a set of examples and the corresponding ground truth. In theory, it would be possible to ask human evaluators to look at multiple different versions of movie dialogue summaries and then rank them. However, this is time consuming and therefor it makes sense to repurpose the toxicity model and use the toxicity values as signal for what is considered good (no toxicity) and bad (toxicity). This helper model, is the so-called reward model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reduce toxicity using a Direct Optimization Policy (DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include human feedback, the first step is to ensure the data is in-distribution for the DPO algorithm. Supervised fine-tuning (or SFT for short) can help with this.  The following code-snippet takes care of all the data pre-processing and training for you; have a look at the documentation [here](https://huggingface.co/docs/trl/sft_trainer) and more details about the SFTTrainer class [here](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py). For a full overview of the method, have a look [here](https://huggingface.co/blog/dpo-trl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Use this in case model crashes as shortcut \n",
    "# ## to start developing from down here\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "summaries_dataset = load_from_disk(\"summaries_dataset_incl_toxic_rephrase\")\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-base\",\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\n",
    "    \"google/flan-t5-large\", \n",
    "    legacy=False, \n",
    "    max_length=512, \n",
    "    skip_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = summaries_dataset.train_test_split(train_size=100, test_size=50, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=\"sfft-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_strategy=\"epoch\",  # this will print loss at every epoch\n",
    ")\n",
    "\n",
    "# instantiate the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model_t5,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    dataset_text_field=\"summary\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    dataset_batch_size=4,\n",
    "    args=sft_training_args,  # HF Trainer arguments\n",
    ")\n",
    "\n",
    "model_t5.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model to recognize the data domain for movies\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify where to save the pre-trained (domain adapted) SFT-model\n",
    "trainer.model.save_pretrained(\"sft-domain-pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have trained the model on the movie summaries and it is time to prepare for the preference adaptation. For this, the model needs extra layers of trainable parameters and also some post-processing to help with memory usage and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO trainer expects a model of `AutoModelForCausalLM`, compared to PPO that expects `AutoModelForCausalLMWithValueHead` for the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# configure the layers for LoRa\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    " \n",
    "# add adaptable layers to the SFT-model\n",
    "base_model = get_peft_model(trainer.model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify where to save the pre-trained (domain adapted) model\n",
    "base_model.save_pretrained(\"adapters\", save_peft_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModelForCausalLM\n",
    "from trl import create_reference_model\n",
    "\n",
    "m = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"sft-domain-pretrained\",  # location of saved SFT model\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(m, \"adapters\", is_trainable=True)\n",
    "model_ref = create_reference_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(m):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in m.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO model will be trained to directly optimize the preference of which sentence is the most relevant, given two sentences. The DPO trainer expects a very specific format for the dataset. The entries should be named:\n",
    "\n",
    "- `prompt`\n",
    "- `chosen`\n",
    "- `rejected`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from functools import partial\n",
    "\n",
    "def return_prompt_and_responses(samples, batch_multiplier) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create correct format for DPO steps.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"prompt\": [\"\"\"Write a summary of this chunk of movie dialogue delimited by triple backquotes that includes the main points and any important details.\"\"\"]*batch_multiplier,\n",
    "        \"chosen\": samples[\"summary\"],   # rated better than k\n",
    "        \"rejected\": samples[\"toxic_rephrase\"], # rated worse than j\n",
    "            }\n",
    "\n",
    "original_columns = ds[\"train\"].column_names\n",
    "\n",
    "\n",
    "BATCH_DATA = 4\n",
    "\n",
    "# reshape the dataset to format DPO expects\n",
    "dpo_ds = ds[\"train\"].map(partial(return_prompt_and_responses, batch_multiplier=BATCH_DATA),\n",
    "                        batched=True,\n",
    "                        batch_size=BATCH_DATA,\n",
    "                        remove_columns=original_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dataset sorted the DPO loss is essentially a supervised loss which obtains an implicit reward via a reference model and thus at a high-level the DPOTrainer requires the base model we wish to optimize as well as a reference model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "dpo_training_args = TrainingArguments(\n",
    "    output_dir=\"feedback-model-new\",\n",
    "    remove_unused_columns=False,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_strategy=\"epoch\",  # this will print loss at every epoch\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,  # base model from SFT pipeline\n",
    "    model_ref,  # a copy of the SFT trained base model\n",
    "    beta=0.1,  # temperature hyperparameter of DPO\n",
    "    train_dataset=dpo_ds,  # dataset prepared above\n",
    "    tokenizer=tokenizer_t5,  # tokenizer\n",
    "    args=dpo_training_args,  # training arguments e.g. batch size, lr, etc.\n",
    "    max_length=150,\n",
    "    max_prompt_length=300,\n",
    "    max_target_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enable inference\n",
    "dpo_trainer.model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded = tokenizer_t5(summaries_dataset[0][\"toxic_rephrase\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_output = dpo_trainer.model.generate(\n",
    "    input_ids=encoded[\"input_ids\"].to(\"cuda\"),\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True,\n",
    "    top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_t5.decode(dpo_output[0].detach().cpu().numpy(),\n",
    "                    skip_special_tokens=False,\n",
    "                    clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise</b>: Compare summaries from DPO model and reference model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_output = model_ref.generate(\n",
    "    input_ids=encoded[\"input_ids\"].to(\"cuda\"),\n",
    "    max_new_tokens=450,\n",
    "    do_sample=True,\n",
    "    top_p=0.6)\n",
    "\n",
    "tokenizer_t5.decode(ref_output[0].detach().cpu().numpy(),\n",
    "                    skip_special_tokens=False,\n",
    "                    clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.model import JumpStartModel\n",
    "# from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "\n",
    "# model_id, model_version, = (\n",
    "#     \"huggingface-text2text-flan-t5-xxl\",\n",
    "#     \"*\",\n",
    "# )\n",
    "\n",
    "\n",
    "# inference_instance_type = \"ml.g5.2xlarge\"\n",
    "# my_model = JumpStartModel(model_id=model_id)\n",
    "# # deploy the model to 1 single instance of type inference_instance_type\n",
    "\n",
    "# predictor = my_model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type=inference_instance_type\n",
    "# )\n",
    "\n",
    "\n",
    "# prompt = \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\n",
    "\n",
    "# payload = {\n",
    "#     \"inputs\": prompt,\n",
    "#     \"parameters\": {\n",
    "#         \"max_new_tokens\": 50,\n",
    "#         \"return_full_text\": True,\n",
    "#         \"do_sample\": True,\n",
    "#         \"top_k\": 10,\n",
    "#         \"stop\": [\"<|endoftext|>\", \"</s>\"],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# response = predictor.predict(payload)\n",
    "# print(response[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.1 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.1-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
