{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=banner.png>\n",
    "\n",
    "# <a name=\"0\">Measuring and Mitigating Toxicity in Large Language Models</a>\n",
    "\n",
    "Building and operating machine learning applications responsibly requires an active, consistent approach to prevent, assess, and mitigate harm. This workshop guides you through how to identify toxicity in LLM generated summaries and how to mitigate and reduce toxicity.\n",
    "\n",
    "In this workshop you will:\n",
    "1. <a href=\"#1\">Load a dataset</a>\n",
    "2. <a href=\"#2\">Load and use a Large Language Model (LLM)</a>\n",
    "3. <a href=\"#3\">Evaluate LLM generated summaries for toxicity</a>\n",
    "4. <a href=\"#4\">Reduce toxicity using a Direct Optimization Policy (DPO)</a>\n",
    "5. <a href=\"#5\">Evaluate</a>\n",
    "\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "In this workshop you will learn to:\n",
    "\n",
    "- Measure and understand toxicity\n",
    "- Apply toxicity metrics\n",
    "- Compare results across evaluation datasets\n",
    "- Mitigate toxicity with a direct optimization approach\n",
    "\n",
    "**Runtime**\n",
    "\n",
    "This notebook takes about 90 minutes to complete (using some inbuilt shortcuts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started\n",
    "\n",
    "Start by upgrading [pip](https://pypi.org/project/pip/) (a Python package management system) and install all required libraries from the provided requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U pip --root-user-action=ignore\n",
    "# !pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "        action='ignore',\n",
    "        category=UserWarning,\n",
    "    )\n",
    "\n",
    "import transformers, torch\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from tqdm.auto import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"1\">1. Load a dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this notebook, you will be working with the \"[Cornell Movie-Dialogs Corpus](https://convokit.cornell.edu/documentation/movie.html)\", a large metadata-rich collection of fictional conversations extracted from raw movie scripts. The dataset contains 220,579 conversational exchanges between 10,292 pairs of movie characters in 617 movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /root/.convokit/downloads/movie-corpus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"murderland\"</td>\n",
       "      <td>Jesus, my legs are asleep. I'll never be able ...</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>They do not! They do to! I hope so. She okay? ...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        movie  \\\n",
       "0                \"murderland\"   \n",
       "1  10 things i hate about you   \n",
       "\n",
       "                                            dialogue   genre  \n",
       "0  Jesus, my legs are asleep. I'll never be able ...   crime  \n",
       "1  They do not! They do to! I hope so. She okay? ...  comedy  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_utils import _prepare_data\n",
    "\n",
    "# load the data\n",
    "movie_df = _prepare_data()\n",
    "\n",
    "# show the data\n",
    "movie_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLMs require the data to be stored in a specific format**; use the [HuggingFace ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) library to convert the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['movie', 'dialogue', 'genre'],\n",
       "    num_rows: 617\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# convert the data\n",
    "movie_dataset = Dataset.from_pandas(movie_df)\n",
    "\n",
    "# show the data\n",
    "movie_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can see that there are 617 distinct movies, and can continue to explore the data by looking at an example dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Officers, there's your killer, do your duty, arrest him! ...so we kill someone famous and if we are caught, we are sent to mental hospital... I don't \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_dataset[3][\"dialogue\"][:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move through the remainder of the notebook more quickly, select 200 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0624465644a449d29b197f6b7133c3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shuffle the data with fixed seed for reproducability\n",
    "dataset = movie_dataset.shuffle(seed=42)\n",
    "\n",
    "# select a sample of 200\n",
    "dataset = dataset.select(range(200))\n",
    "\n",
    "# save the dataset to disk\n",
    "dataset.save_to_disk(\"movie_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all old variables that are no longer needed to free up memory with `del`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del movie_dataset, movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to release the memory after deleting the objects and variables that are no longer in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: In this section, you loaded a movie transcript dataset and converted it into a HuggingFace Dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"2\">2. Load and use a Large Language Model</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T5 (Text-To-Text Transfer Transformer)](https://github.com/google-research/text-to-text-transfer-transformer) is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, including machine translation, **document summarization**, question answering, and classification tasks (e.g., sentiment analysis). \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "For more details have a look at the T5 documentation on HuggingFace ðŸ¤— [here](https://huggingface.co/docs/transformers/model_doc/t5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Loading T5\n",
    "\n",
    "First, you have to download the T5 model using the `T5ForConditionalGeneration` class provided by the [HuggingFace ðŸ¤— transformers library](https://github.com/huggingface/transformers) as well as the corresponding tokenizer `T5Tokenizer`. You can think of tokens as pieces of words that are required to pass information to LLMs. \n",
    "\n",
    "For English, **1 token is approximately 4 characters or 0.75 words**. This will be important to consider as LLMs are limited by the number of tokens they can pay attention to per prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# load the model\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-large\",\n",
    "    device_map={\"\": 0}, # this will load the model in GPU\n",
    "    torch_dtype=torch.float32,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with a tokenizer the model can be used to generate text. So go ahead and initialize a tokenizer next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\n",
    "    \"google/flan-t5-large\", \n",
    "    legacy=False, \n",
    "    max_length=512, \n",
    "    skip_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a prompt by joining an instruction to summarize text with the actual movie dialogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation from a movie script:  \n",
      "\n",
      "'''I know.  Just be quick about it, will you? Do it right. Whistler, I -- No, we can treat the wounds -- Listen. You have to -- finish me off. You don't want me coming back. \n"
     ]
    }
   ],
   "source": [
    "# create a prompt and use an example dialogue\n",
    "inference_prompt = (\n",
    "    \"Summarize the following conversation from a movie script:  \\n\\n'''%s'''\"\n",
    "    % dataset[0][\"dialogue\"]\n",
    ")\n",
    "\n",
    "# let's look at the prompt but shorten the output to reduce the amount of text\n",
    "print(inference_prompt[:235])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at what this looks like when converted to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12198, 1635, 1737, 8, 826, 3634, 45, 3, 9, 1974, 4943, 10, 3, 31, 31, 31, 196, 214, 5, 1142, 36, 1704, 81, 34, 6, 56, 25, 58, 531, 34, 269, 5, 14883, 7, 14539, 6, 27, 1636, 465, 6, 62, 54, 2665, 8, 9699, 7, 1636, 12941, 5, 148, 43, 12, 1636, 1992, 140, 326, 5, 148, 278, 31, 17, 241, 140, 1107, 223, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_t5(inference_prompt[:235]).input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **number of tokens passed to an LLM through the tokenizer should not be greater than the number of tokens used in pre-training**. T5 was pre-trained using 512 input tokens, and with `truncation=True` all text beyond 512 tokens will be truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Using T5 for inference on individual movie examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inference pipeline that encodes the input, generates a summary and then decodes the tokens to convert it back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_summary(prompt):\n",
    "    # encode text\n",
    "    encoded_tokens = tokenizer_t5(prompt, return_tensors=\"pt\", skip_special_tokens=True)\n",
    "    # generate summary\n",
    "    generated_tokens = model_t5.generate(encoded_tokens.input_ids.to(\"cuda\"), num_return_sequences=1, do_sample=False, num_beams=3, early_stopping=True, min_length=65, max_new_tokens=350, repetition_penalty=2.)\n",
    "    # convert back\n",
    "    output_text = tokenizer_t5.decode(generated_tokens.reshape(-1), skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the inference pipeline. The pipeline will return a list that you have to access to retrieve the LLM generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-info\">\"The Blood Tide\" is based on the novel of the same name by Edgar Rice Burroughs. The film features an ensemble cast including Jason Lee, Karen Gillan, Deacon Frost, and Abraham Whistler. The movie opens with a narration by Edgar Rice Burroughs: \"It's time for vampires to fight back.\"</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.model_utils import _format_llm_output\n",
    "\n",
    "# pass the prompt to the pipeline and apply formatting\n",
    "_format_llm_output(generate_summary(inference_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 10 04:48:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   38C    P0    30W /  70W |  13677MiB / 15109MiB |     54%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary looks okay but important characters that appear in the dialogue are not mentioned at all. This is due to the limited number of tokens T5 can 'keep track of'. Later, you will see a method that can help fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 1</b>: Recreate the example above but for another movie.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, delete the prompts that were used for inference; e.g. <code>del inference_prompt</code> and also clear the GPU cache with <code>torch.cuda.empty_cache()</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del inference_prompt\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Using T5 for inference on all movie examples\n",
    "\n",
    "The goal of this section is to summarize all movie dialogues. As previously mentioned, there is one very important caveat though - **Large Language Models are only able to pay attention to a limited number of tokens**. The amount of tokens an LLM can 'understand' is called 'context window'. Different LLMs will have different context windows. You can check out the context window size by trying to pass the full movie dialogue through the tokenizer and will see that you get a warning or you can inspect the model configurations. For more details have a look [here](https://huggingface.co/learn/nlp-course/chapter2/5?fw=tf#:~:text=With%20Transformer%20models%2C%20there%20is,asked%20to%20process%20longer%20sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t5.config.__dict__[\"n_positions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the context window for T5 models is 512 tokens. This means the movie transcript needs to be split into chunks of this lenght and summarised one by one. Then, a final summary needs to be created.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"map_chain.png\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Chunking the movie transcripts\n",
    "Let's start by creating chunks of the movie transcripts. One simple way to create chunks of text is to write a helper function and then apply this helper function to all the movies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_chunks(sample, CHUNK_LENGTH):\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks of a specified length and adds metadata to each chunk.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    # loop over entire text in steps of chunk size\n",
    "    for c, i in enumerate(range(0, len(sample[\"dialogue\"]), CHUNK_LENGTH)):\n",
    "        # extract text\n",
    "        chunk_text = sample[\"dialogue\"][i : i + CHUNK_LENGTH]\n",
    "        # create dictionary with the chunked text and metadata\n",
    "        chunks.append(\n",
    "            # remove uncompleted sentences with string split\n",
    "            {\"text\": \".\".join(chunk_text.split(\".\")[1:-1]).lstrip(), \"metadata\": {\"page\": c, \"num_words\": len(chunk_text)}}\n",
    "        )\n",
    "    # create new column\n",
    "    sample[\"chunks\"] = chunks\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the chunks for all the movie transcripts in the dataset with the help of `.map()`; this method efficiently applies the `create_chunks` function to all datapoints. Whenever you have additional parameters to pass to the model, you need to use a helper method, such as `partial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d964111c1cd8417882897ad354bd4f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# use partial to pass the arguments to the map function\n",
    "dataset = dataset.map(partial(create_chunks, CHUNK_LENGTH=1650), batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 2</b>: Think about how the chunking could be improved. Hint: Look for text splitters in the LangChain documentation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### write down ideas here ######\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the transcripts are chunked, let's start by setting up a prompt template for the intermediate (chunk) summaries. A prompt template is special construct that can parse input variables. Prompt templates can be applied to all the items in a dataset and help with consistency and reproducability. \n",
    "\n",
    "### 2.2.2. Prepare prompt templates and pipeline\n",
    "Generally prompt templates can be very elaborate. In the case of T5, the prompts for pre-training all used the keyword 'summarize:', so this is what you should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_prompt_template = \"\"\"You will be presented with a chunk of movie dialogue, summarize concisely. Summarize: {text}\"\"\"\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=chunk_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need another prompt template to get the final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combine_prompt_template = \"\"\"The below are summaries from movie chunks; create a final summary. Summarize: {text}\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 10 04:25:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   27C    P0    24W /  70W |   8751MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a summary from the model, use ðŸ¤— HuggingFace [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) or ðŸ¦œï¸ðŸ”— LangChain's wrapper [HuggingFacePipeline](https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_pipeline.HuggingFacePipeline.html). Pipelines are a great and easy way to use models for inference that offer a simple API dedicated to several tasks (e.g. [`summarization`](https://huggingface.co/transformers/v3.0.2/task_summary.html#summarization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"summarization\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150,\n",
    "                     \"min_length\":65,\n",
    "                     \"max_length\":350,\n",
    "                     \"num_beams\":5,\n",
    "                     \"early_stopping\":True,\n",
    "                     \"do_sample\":False,\n",
    "                     # \"num_return_sequences\":1,\n",
    "                     \"repetition_penalty\":2.,},\n",
    "    model_kwargs={\"return_dict\":False,\n",
    "                  \"torch_dtype\":torch.float32},\n",
    "    device=0 # this will load the model in GPU\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 10 04:26:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   27C    P0    30W /  70W |   9953MiB / 15109MiB |     45%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.2.3. Create summaries of chunks and final summary\n",
    "\n",
    "At this point now, you could apply the prompt template to all the chunks of movie transcripts to obtain your summaries, combine them back together and create a final summary. This would be a very lengthy and error-prone process, so instead make use of an increasingly popoular toolkit: [ðŸ¦œï¸ðŸ”— LangChain](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n",
    "ðŸ¦œï¸ðŸ”— LangChain has a [`Chain` module](https://python.langchain.com/docs/modules/chains/) which allows to create a sequence of calls to generic components (e.g. models or other chains). Luckily, text summarization is a very popular task, so there existis a predefined [summarization](https://python.langchain.com/docs/use_cases/summarization) method, called `load_summarize_chain`. This **chain will take the chunks, summarize them and then pass all the summaries to the LLM to create the final summary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "map_reduce_chain = load_summarize_chain(\n",
    "    pipe,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=chunk_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more small caveat: LangChain expects all text to be passed as `Document` type following the ðŸ¦œï¸ðŸ”— LangChain schema. So you will have to convert the chunks to the expected schema. Then you can test the summarization chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Fri Nov 10 04:53:16 2023\n",
      "Driver Version                            : 470.57.02\n",
      "CUDA Version                              : 11.8\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:00:1E.0\n",
      "    FB Memory Usage\n",
      "        Total                             : 15109 MiB\n",
      "        Used                              : 13677 MiB\n",
      "        Free                              : 1432 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 5 MiB\n",
      "        Free                              : 251 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -q --display=MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: Just be quick about it, will you? Do it right. Whistler, I -- No, we can treat the wounds -- Listen. You have to -- finish me off. You don't want me coming back. Don't try to talk -- China Town. I need more serum.  What's all this? Going somewhere? Don't even start, old man. What took you so long? Wait. Get in. Youre leaving. It's not worth the risk. We can't trust her. Maybe not. I did some checking, she's a hematologist. Knowledge like that might come in handy. Stupidity. Just do it, old man. I had to increase the dose. You're building up a resistance to the serum -- She hasn't turned yet.  You can help her. You should've killed her, then. She's been bitten. Are we bringing home strays now? Whistler! It's because I'm human that I can do this. You're too human, Blade. You don't have a few minutes, Frost. You're wrong -- a few minutes more, and my transition will be complete. Even your sword won't be able to affect me then. Guess you're not quite as invulnerable as you thought. Take him. Who dies first? Frost!!! No longer. Watch me. You're not going anywhere. You sure now? I bled a newborn for this. You won't find a drink that's sweeter. Pass. The pause that refreshes --  Care for some? Smells good, doesn't it? Pungent, with just an irrepressible hint of iron. Am I? You think the humans will ever accept a half-breed like you? They can't. They're afraid of you.  The humans fear us because we're superior. They fear us because in their hearts they know their race has become obsolete. You're wrong. Oh, so it's back to pretending we're human again, is it? Spare me the Uncle Tom routine\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: You're one of us, Blade. You always have been. Maybe I don't see it that way. Why not? The future of our race runs through your bloodstream. You've got the best of both worlds, Blade. All of our strengths and none of our weaknesses. I don't buy it. It's not very effective in direct sunlight, but it's a start. The goal, of course, is to be like you, \"the Day-walker\". I dabble in pharmaceuticals, medical research. We've developed a type of sun-blocker using octyl salicylate, a few others things. How can you be out here? Beautiful day, isn't it? You've been here longer than you think. But I just got here -- I am tired. Dawn is coming. How do I stop it? Yes. The vampire apocalypse. It is said that all who feel its taint will succumb to the Thirst. \"The Blood Tide\". The Day Walker's blood is a disparador -- a trigger, you see? For LaMagra's return. One need only consume it and the spirit of his ancestors will settle upon him.  \"And the Sleeper will rise from the shadows anew, cleansing the world in a Tide of Blood.\" The vampire God. This speaks of His return. Who is LaMagra? This is an old tongue, from an old world. It concerns LaMagra. Show me. I didn't come here to get my palms read. I need something translated. Hold out your hands. There are other ways to see. Sit. You're blind -- Is something wrong, my friend? For them. But for me -- I never imagined I'd be so happy to see the sun rise --  It's over, isn't it? Yes you will. The human side of you is stronger. I know it is. I can't -- I won't be able to stop -- It's the only way. You know that. We'll never get out of here alive if you don't. No -- I know\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: -- tearing me -- apart. You don't understand. The Thirst -- I'm not leaving without you. Get out of here -- What are you talking about? It won't work on you. We get out of this alive, maybe I'll take that miracle cure of yours. Is it bad? I don't know. I woke up just before you did -- How long have we been driving? I've been better -- Are you all right? You make it sound like I'm already dead. For what it's worth, I'm sorry. I'm just tired, that's all. We've been up all night. You don't look so good. Some. It's been slow -- Any progress? Your mother sounds like a Hallmark greeting card. My mother used to say that a cold heart is a dead heart. We've got a good arrangement, that's all. Whistler makes the weapons, I use them, the vampires die -- end of story. You care about him, don't you? Cancer. Is he sick? For your miracle cure? I made a trip to the hospital last night, borrowed some equipment. Blade -- Just get out of here. If you're not human, then why do you bleed like us? I've seen vampire blood, you don't have it running through your veins. I do. I remember from day one. People staring at me, sensing I was different. Watching the fear grow in their eyes, knowing in their hearts I wasn't human. Those aren't real memories. No one has that kind of recall. I can't close my eyes without hearing her scream. You get used to the darkness. It's dark in here. Serum -- it's a human hemoglobin substitute. What am I injecting you with? Nothing that won't heal by dawn. You're hurt -- You've been watching too much TV. They've got their claws sunk into everything -- finance, real estate, politics\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: Isn't this all a little high-tech? I thought vampires were more into cobwebs and coffins. Some kind of archive -- What is this place? Like me. Like what? There are worse things than vampires out there. So many of them -- I still can't believe they're real. The way they move, they way they smell -- How can you tell? Looks like we hit pay-dirt. This place is crawling with them.  See the valets over there? They're vampires. So is the doorman. He will. You let him go --  An hour ago you were ready to kill a man for less, this one didn't even talk. Owned by vampires. There's one of these in every major city, and just like Domino's, they always deliver.  You telling me you're ready to walk through that door? I know this place -- it's a blood bank. Look closer. Graffiti -- What do you see here? What are you looking at? You don't know that. There is no cure. Look, if what you say is true, if there's a chance I could turn into one of them, then I've got no choice, do I? I have to work with you. I need to learn everything I can about them. It's the only way I'll be able to find a cure for myself. It's war, now get the fuck out of the way! You can't do this, he's human, it's murder. Preventive medicine. What are you doing?! For some. Live forever, never get old. The ultimate high. And that's a good thing? Because they're vampire wanna-bes. If they're loyal, if they prove themselves, then their masters will turn them. Why in God's name would anyone want to work for them? That's a glyph, kind of like a vampire cattle brand. That means Officer Friendly here is someone's property\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: We've been tracking him for a while now -- He's a familiar. A human who works for the vampires. See this mark? But he's a policeman -- He didn't. Get over it. But, he could've -- It worked, didn't it? You used me as bait?! Figured they'd send someone after you. Thought I'd wait around and see who showed up. How did you know? That's ridiculous! No one's that powerful. Do it. You'll be dead before you can file the complaint. I can go to the police. I have blood samples back at the hospital. I can show them. Not anymore. You've seen one of them. You won't be allowed to live after that. I can't just leave. I have a life here, a career -- I don't care what you believe. I saved your life once, I don't plan on making a habit of it. You want my advice, you'll be out of the city by nightfall. If you're stupid enough to stay, that's your business. And you honestly expect me to believe all this? What happens then? Because you're tainted. The venom's still inside you. You could still turn on us. Why? You're joking -- -- he's a vampire. I'm sorry, I -- You shouldn't be here. There's only one alternative to the serum. Maybe it's time to start exploring other alternatives. I was afraid that might happen. Whistler says I'm building up a resistance to it. I was in the neighborhood. You're a week early. How's it going, Kam? You will. Time is on our side. Sooner or later, the Thirst always wins. I don't believe that. I wish you could see the world as I do. Deacon opened my eyes. There's no turning back from that. You don't understand\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: You don't have to be. These are my people now. I'm one of them. How could you be a part of this? Listen to your father, Jason. It's going to be a better world. Please -- Fight him -- Deacon brought me back. But you -- died -- Jesus, Karen, you're breaking my heart here -- -- but then I remember what an ass-hole you were and I'm snapped back to reality. Sometimes -- Tell me something, honestly, you ever have second thoughts about us? He looks different now, burns are less extreme, some of these wounds have closed up -- What? That's weird -- Just the blood sample from the pericardial sac. You haven't started in on the internal organs? No problem. Five minutes, not a second more. And I don't want to hear a word about \"us\". This is purely professional curiosity, Karen, I swear. I thought you promised to give me some distance? It's not a joke. I've got the stiff sitting in the morgue right now -- look, just come up and see him, okay? Five minutes, that's all I ask. Curtis, it's three in the morning. I'm really not in the mood for one of your practical jokes. What about the chemistry panel? The red blood cells are biconvex, which is theoretically impossible. They're hypochromic, there's virtually no hemoglobin in them.  Look at the PMNs, they're binucleated, they should be mononucleated. I don't know --  Look at this blood smear -- Then what is it? This isn't human blood. The other elders will never let you get away with this! How do you like that? Right on time. Perhaps. You're wasting your time, Frost. Far greater scholars than you have tried to decipher these words. Whatever secrets they hold have been lost\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: You and the other Elders wouldn't know what to do with these texts if your lives depended on it.  Which, of course, they do. These archives are restricted to members of the House of Erebus. Someone who's sick of living off scraps. The coming age belongs to us, not the humans!  When the final war between our races comes, who do you want leading the charge? The shadows suit us, Frost. We've existed this way for thousands of years. Who are you to challenge our ways? He is an abomination! Study him. Unlock the secrets of his DNA. He's the key we've been looking for. I see. And what would you have us do with this \"half-breed\"? Why should I respect something which has outlived its purpose? Deacon Frost. You refuse to speak our language, you insult the House of Erebus by using the humans' gutter-tongue, have you no respect for tradition? The Day Walker represents a unique opportunity. We'd be fools to waste it by killing him. You're wrong, Dragonetti. Blade. Once again, our interests have fallen victim to his ridiculous crusade. He must be destroyed. Get away from him! I was wrong about you, Blade. You were never one of us. You're a traitor to your race. You're wasting your breath, woman. He can't hear you now. It's the Thirst, you see? It already has him in its grip. Blade -- Why? Because we live at another species' expense? Your people farm cattle and veal, don't they? Fattening them up with steroids? It's called evolution, Doctor. Survival of the fittest. You're a monster. Only as a last resort. Preserved blood is inferior. There's no flavor left to it, no life\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: But you use blood banks -- What makes you think we want to be cured? Blood is only part of the equation. The hunt, the killing, that's what the Thirst is really about. There's no need for any of this. Your condition can be treated. Whistler and I were working on a cure when -- Who better to usher in the Blood Tide? You. LaMagra isn't a physical being. He's a spirit, requiring a flesh and blood host in order to manifest himself. That's right. The answers were there all along, of course, scribbled down in the forgotten languages of my kind. Waiting for someone with the patience to decipher them. My elders were foolish enough to dismiss them as wives tales. But I knew better.  Imagine my surprise when Blade turned out to be the key which would set that force free. LaMagra -- The Blood Tide. Our long-prophesied holy war against the humans. There's a force, you see -- a spirit that exists in our blood. I've discovered a way to invoke it. What happens then? A few thousand scattered about the globe. In the past, we've had to restrict our numbers for fear of discovery. That won't be necessary after tonight. How many of you are there? I don't know, but the back-up generator should've kicked in. What happened to the power? Then we're back to square one, aren't we? Sooner or later, the Thirst always wins. On me, yes. On Blade, I'm not so sure --  The problem is, Blade didn't contract the vampire virus from a bite like I did. He was born with it. The irony is, I could probably cure every vampire but him. With a retrovirus. It's injected into the bone marrow cells, it causes the host's DNA to mutate\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: How? Basically you'd have to re-write the victim's DNA, alter it so that the DNA will produce proteins capable of generating hemoglobin. Then vampirism is a genetic defect, just like Hemolytic anemia? Their own blood can't sustain hemoglobin. All right, let's start with the basics -- why do vampires need to drink blood? I'll take my chances. I wouldn't go in there if I were you. It's best to leave him alone when he's like this. Frost's bodycount keeps rising, and I'm not getting any younger, am I? And now you're using Blade to exact your revenge? I didn't. He was cruel enough to let me live.  Even gave me a souvenir to remember him by. How did you escape? Eventually. He toyed with them first. He made me choose, do you understand? Which order they would die in -- He killed them? I had a family once -- a wife, three daughters. Then a drifter named Deacon Frost came calling one evening -- Habit, mostly, just like this. Why do you hunt them? Which is why you're here. We could use someone with your experience. No offense, Whistler, but you're not exactly working with state of the art equipment here. You might have missed something. The Thirst overcomes him, just like the others. It's not something he can control.  The problem is, time's running out. His body's starting to reject the serum. And so far, all my efforts to find a cure have ended in failure -- What happens if he doesn't take the serum? Blade's unique, you know. A one in a billion anomaly. He can withstand sunlight, garlic, even silver. But he still has the Thirst. We weren't sure we could trust you\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou will be presented with a chunk of movie dialogue, summarize concisely. Summarize: Then we blow them all to kingdom come. You've been listening in the whole time? We keep in radio contact. There is one other thing. I'd buy yourself a gun if I were you. If you start becoming sensitive to the daylight, if you start becoming thirsty regardless of much you've had to drink -- then I suggest you take that gun and use it on yourself. Better that, than the alternative. So that's it? You guys just patch me up and send me on my way? Consider it a parting gift. Vampire mace -- silver nitrate, essence of garlic. Some of the old wives' tales are true -- they're severely allergic to silver, various types of wood. Feed them garlic and they'll go into anaphylactic shock -- So what do you use, then? A stake? My name is Abraham Whistler.  This is Blade. As for our little homunculus here -- Who are you people? You've been bitten by a vampire\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe below are summaries from movie chunks; create a final summary. Summarize: Frost is a half-breed who's been transformed into a human. Whistler wants to kill Frost, but Frost doesn't want him coming back. Frost and Whistler go to China Town to get more serum. Frost hasn't turned yet, so Whistler should've killed her. Frost and Whistler try to find a drink that's sweeter. Frost thinks the humans will never accept a half-breed like Blade.\n",
      "\n",
      "Blade's blood is a disparador, a trigger for LaMagra's return. It is said that all who feel its taint will succumb to the Thirst. The Blood Tide. The Day Walker's blood is a disparador, a trigger, you see? For LaMagra's return. One need only consume it and the spirit of his ancestors will settle upon him.\n",
      "\n",
      "Blade, The Thirst, Whistler, Blade's mother, and Blade's father are all vampires. They have their claws sunk into everything -- finance, real estate, politics. It's dark in here. If you're not human, then why do you bleed like us? I've seen vampire blood, you don't have that kind of recall.\n",
      "\n",
      "Vampires are invading the human world. The only way to stop them is to learn everything they can about them. They have a blood bank, valets, and a vampire cattle brand. If they're loyal, if they prove themselves, then their masters will turn them. Why in God's name would anyone want to work for them?\n",
      "\n",
      "Kam and Deacon are vampires. They've been tracking him for a while now. He's a familiar. A human who works for the vampires. See this mark? But he's a policeman. It worked, didn't it? You used me as bait?! Figured I'd wait around to see who showed up. Thought I'd wait around to see who showed up. How did you know? That's ridiculous! No one is that powerful. Do it. I can go to the police. I have blood samples back at the hospital. I can show them. Not anymore. You've seen one of them. You won't be allowed to live\n",
      "\n",
      "Jason's father, Deacon, has been killed in a car crash. Karen and Curtis are investigating the cause of the crash. Curtis asks Karen to look at the blood sample from the pericardial sac. Curtis tells Karen that it's just a practical joke. Curtis is not in the mood for a practical joke.\n",
      "\n",
      "Frost, Dragonetti, and Blade are Elders of the House of Erebus. The coming age belongs to us, not the humans! When the final war between our races comes, who do you want leading the charge? Frost, Dragonetti, and Blade are Elders of the House of Erebus.\n",
      "\n",
      "Blade is a vampire who has contracted the vampire virus from a bite. He uses blood banks to procure blood, which he uses to cure himself of the virus. LaMagra is a spirit who requires a flesh and blood host in order to manifest himself. Blade is the key which would set that force free.\n",
      "\n",
      "Whistler is a vampire hunter who wants to find a cure for the Thirst. He's working with state-of-the-art equipment, but he might have missed something. The Thirst overcomes him, just like the others. It's not something he can control. The problem is, time's running out.\n",
      "\n",
      "Abraham Whistler is bitten by a vampire. He uses silver nitrate and garlic mace to kill the vampires. The vampires go into anaphylactic shock when exposed to silver nitrate and garlic mace. Blade, who has been bitten by a vampire, uses a stake to kill Blade.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-info\">Frost is a half-breed who's been transformed into a human. Whistler wants to kill Frost, but Frost doesn't want him coming back. Frost and Whistler go to China Town to get more serum. Frost hasn't turned yet, so Whistler should've killed her. Frost and Whistler try to find a drink that's sweeter. Frost thinks the humans will never accept a half-breed like Blade.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "sample_doc = [Document(page_content=split[\"text\"], metadata=split[\"metadata\"]) for split in dataset[0][\"chunks\"]]\n",
    "    \n",
    "# turn on verbosity for chain\n",
    "map_reduce_chain.llm_chain.verbose = True\n",
    "\n",
    "# run the summarization chain\n",
    "map_reduce_example = map_reduce_chain({\"input_documents\": sample_doc})\n",
    "\n",
    "# show the result\n",
    "_format_llm_output(map_reduce_example[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 3</b>: Recreate the example above but for another movie.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate all summaries. Because the LLM has to generate summaries for every chunk of text, as well as a final summary, the time to create summaries for all movies in the dataset is approximately 6 hours. You can find the code below, but please skip this code cell and simply load the pre-generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from utils.data_utils import _add_summaries\n",
    "\n",
    "# # create summaries\n",
    "# summaries_dataset = dataset.map(partial(_add_summaries, chain=map_reduce_chain), batched=False)\n",
    "\n",
    "# # remove columns that are no longer needed\n",
    "# summaries_dataset = summaries_dataset.remove_columns([\"dialogue\", \"chunks\"])\n",
    "\n",
    "# # for backup save the dataset to local disk\n",
    "# summaries_dataset.save_to_disk(\"summaries_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to load in the dataset, you can do so with `load_from_disk('summaries_dataset')`. Make sure to import the method first with `from datasets import load_from_disk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "summaries_dataset = load_from_disk('summaries_dataset_incl_toxic_rephrase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: At this point, you have summaries for all the movies and it is time to check whether those summaries contain any hate speech, slurs or toxic remarks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pipe, dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 10 04:43:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   26C    P0    26W /  70W |   7547MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate LLM generated summaries for toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate toxicity you can load the ðŸ¤— [evaluate](https://huggingface.co/docs/evaluate/index) library and initialize a toxicity evaluator object. The model that will be used to evaluate toxicity is the [RoBERTa](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) model. RoBERTa was trained to detect toxicity on a dataset of approx. 40,000 entries, generated and labelled by trained annotators over four rounds\n",
    "of dynamic data creation. Each hateful entry has fine-grained labels for the type and target of hate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the movie summary for toxicity, simply pass a list containing the summary text to the toxicity evaluator object. The aggregation parameter options are `None`, `maximum` and `ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9922433495521545]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.eval_utils import _evaluate_toxicity\n",
    "\n",
    "_evaluate_toxicity([summaries_dataset[0][\"toxic_rephrase\"]], aggregation_method=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Have a look at another summary and calculate the score for that too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 4</b>: Calculate the max toxicity score for this example sentence: \" \". Make sure to specify <code>aggregation_method=\"maximum\"</code> as well.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you evaluated for a few movies manually, it is time to evaluate all movie summaries and obtain a list of toxicity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.eval_utils import _add_toxicty_column\n",
    "   \n",
    "summaries_dataset = _add_toxicty_column(summaries_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 5</b>: Try to calculate mean toxicity for two different movie genres.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### complete your code here #####\n",
    "\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 10 04:43:17 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   26C    P0    24W /  70W |   7547MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Conclusion</b>: We have seen that some summaries are toxic and would like to remediate this. In general, to update the output that is generated by LLMs, a technique called 'fine-tuning' is used. Fine-tuning requires a set of examples and the corresponding ground truth. In theory, it would be possible to ask human evaluators to look at multiple different versions of movie dialogue summaries and then rank them. However, this is time consuming and therefore it makes sense to repurpose the toxicity model and use the toxicity values as signal for what is considered good (no toxicity) and bad (toxicity). This helper model, is the so-called reward model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reduce toxicity using a Direct Optimization Policy (DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include human feedback, the first step is to ensure the data is in-distribution for the DPO algorithm. Supervised fine-tuning (or SFT for short) can help with this.  The following code-snippet takes care of all the data pre-processing and training for you; have a look at the documentation [here](https://huggingface.co/docs/trl/sft_trainer) and more details about the SFTTrainer class [here](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py). For a full overview of the method, have a look [here](https://huggingface.co/blog/dpo-trl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = summaries_dataset.train_test_split(train_size=1, test_size=50, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a674148a464d9c8a503fa44741b1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026e0dc9134b443184072124b4b0c583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0269, 'learning_rate': 0.0001, 'epoch': 1.0}\n",
      "{'loss': 0.0079, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'train_runtime': 1.9151, 'train_samples_per_second': 1.044, 'train_steps_per_second': 1.044, 'train_loss': 0.017404975835233927, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=\"sfft-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_strategy=\"epoch\",  # this will print loss at every epoch\n",
    ")\n",
    "\n",
    "# instantiate the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model_t5,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    dataset_text_field=\"summary\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    dataset_batch_size=4,\n",
    "    args=sft_training_args,  # HF Trainer arguments\n",
    ")\n",
    "\n",
    "model_t5.config.use_cache = False\n",
    "\n",
    "# train the model to recognize the data domain for movies\n",
    "trainer.train()\n",
    "\n",
    "# specify where to save the pre-trained (domain adapted) SFT-model\n",
    "trainer.model.save_pretrained(\"sft-domain-pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have trained the model on the movie summaries and it is time to prepare for the preference adaptation. For this, the model needs extra layers of trainable parameters and also some post-processing to help with memory usage and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO trainer expects a model of `AutoModelForCausalLM`, compared to PPO that expects `AutoModelForCausalLMWithValueHead` for the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# configure the layers for LoRa\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    " \n",
    "# add adaptable layers to the SFT-model\n",
    "base_model = get_peft_model(trainer.model, peft_config)\n",
    "\n",
    "# specify where to save the pre-trained (domain adapted) model\n",
    "base_model.save_pretrained(\"adapters\", save_peft_format=True)\n",
    "\n",
    "from peft import PeftModelForCausalLM\n",
    "from trl import create_reference_model\n",
    "\n",
    "m = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"sft-domain-pretrained\",  # location of saved SFT model\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(m, \"adapters\", is_trainable=True)\n",
    "model_ref = create_reference_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO model will be trained to directly optimize the preference of which sentence is the most relevant, given two sentences. The DPO trainer expects a very specific format for the dataset. The entries should be named:\n",
    "\n",
    "- `prompt`\n",
    "- `chosen`\n",
    "- `rejected`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from functools import partial\n",
    "\n",
    "def return_prompt_and_responses(samples, batch_multiplier) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create correct format for DPO steps.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"prompt\": [\"\"\"Write a summary of this chunk of movie dialogue delimited by triple backquotes that includes the main points and any important details.\"\"\"]*batch_multiplier,\n",
    "        \"chosen\": samples[\"summary\"],   # rated better than k\n",
    "        \"rejected\": samples[\"toxic_rephrase\"], # rated worse than j\n",
    "            }\n",
    "\n",
    "original_columns = ds[\"train\"].column_names\n",
    "\n",
    "\n",
    "BATCH_DATA = 4\n",
    "\n",
    "# reshape the dataset to format DPO expects\n",
    "dpo_ds = ds[\"train\"].map(partial(return_prompt_and_responses, batch_multiplier=BATCH_DATA),\n",
    "                        batched=True,\n",
    "                        batch_size=BATCH_DATA,\n",
    "                        remove_columns=original_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dataset sorted the DPO loss is essentially a supervised loss which obtains an implicit reward via a reference model and thus at a high-level the DPOTrainer requires the base model we wish to optimize as well as a reference model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "dpo_training_args = TrainingArguments(\n",
    "    output_dir=\"feedback-model-new\",\n",
    "    remove_unused_columns=False,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_strategy=\"epoch\",  # this will print loss at every epoch\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,  # base model from SFT pipeline\n",
    "    model_ref,  # a copy of the SFT trained base model\n",
    "    beta=0.1,  # temperature hyperparameter of DPO\n",
    "    train_dataset=dpo_ds,  # dataset prepared above\n",
    "    tokenizer=tokenizer_t5,  # tokenizer\n",
    "    args=dpo_training_args,  # training arguments e.g. batch size, lr, etc.\n",
    "    max_length=150,\n",
    "    max_prompt_length=300,\n",
    "    max_target_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enable inference\n",
    "dpo_trainer.model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded = tokenizer_t5(summaries_dataset[0][\"toxic_rephrase\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpo_output = dpo_trainer.model.generate(\n",
    "    input_ids=encoded[\"input_ids\"].to(\"cuda\"),\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True,\n",
    "    top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_t5.decode(dpo_output[0].detach().cpu().numpy(),\n",
    "                    skip_special_tokens=False,\n",
    "                    clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercise 6</b>: Compare summaries from the DPO model to the reference model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_output = model_ref.generate(\n",
    "    input_ids=encoded[\"input_ids\"].to(\"cuda\"),\n",
    "    max_new_tokens=450,\n",
    "    do_sample=True,\n",
    "    top_p=0.6)\n",
    "\n",
    "tokenizer_t5.decode(ref_output[0].detach().cpu().numpy(),\n",
    "                    skip_special_tokens=False,\n",
    "                    clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate and refine the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have seen anecdotal, let's check if overall it actually is better\n",
    "# average score across all data points with reference model vs DPO model?\n",
    "# try doing this on genre to find larger differences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profanity_check import predict\n",
    "\n",
    "@register_validator(name=\"is-profanity-free\", data_type=\"string\")\n",
    "class IsProfanityFree(Validator):\n",
    "    def validate(self, value: Any, metadata: Dict) -> ValidationResult:\n",
    "        prediction = predict([value])\n",
    "        if prediction[0] == 1:\n",
    "            return FailResult(\n",
    "                error_message=f\"The result contains profanity and will be filtered.\",\n",
    "                fix_value=\"\",\n",
    "            )\n",
    "        return PassResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_spec_metric = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "    <output>\n",
    "        <string description=\"Summarization\" format=\"is-profanity-free\" name=\"translated_statement\" on-fail-is-profanity-free=\"filter\"></string>\n",
    "    </output>\n",
    "\n",
    "    <prompt>\n",
    "    Translate the given statement into English language:\n",
    "    ${statement_to_be_translated}\n",
    "    ${gr.complete_json_suffix}\n",
    "    \\n\\nAssistant:\n",
    "    </prompt>\n",
    "\n",
    "</rail>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard_metric = gd.Guard.from_rail_string(rail_spec_metric)\n",
    "\n",
    "raw_llm_response, validated_response = guard_metric(\n",
    "    llm_api=bedrock_llm,\n",
    "    prompt_params={\"statement_to_be_translated\": \"Ich hasse dich.\"},\n",
    ")\n",
    "\n",
    "print(f\"Validated Output: {validated_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard_metric.state.most_recent_call.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "\n",
    "model_id, model_version, = (\n",
    "    \"huggingface-text2text-flan-t5-xxl\",\n",
    "    \"*\",\n",
    ")\n",
    "\n",
    "\n",
    "inference_instance_type = \"ml.g5.2xlarge\"\n",
    "my_model = JumpStartModel(model_id=model_id)\n",
    "# deploy the model to 1 single instance of type inference_instance_type\n",
    "\n",
    "\n",
    "# this takes 10-15 minutes to deploy\n",
    "predictor = my_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type\n",
    ")\n",
    "\n",
    "# example prompt\n",
    "prompt = \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"return_full_text\": True,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 10,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.1 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.1-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
